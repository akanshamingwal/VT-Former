{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnmQ2tpX2eDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54cd143a-7e51-44fc-c3ea-cd8b4bea33cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt20cu118)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt20cu118)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch-geometric) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RomJ1dSwYNP7",
        "outputId": "cd10c62c-8dd6-474d-a735-4cc0827b9379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('/content/drive/MyDrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrdR512FAo4z",
        "outputId": "4de72797-704a-42bd-b88c-89c8b075774e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['242IS003_Assignment1.zip',\n",
              " '242IS003_Assignment2.zip',\n",
              " 'Colab Notebooks',\n",
              " '242IS003_Assignment3.zip',\n",
              " 'Dataset',\n",
              " 'Copy of Copy of VT-Former.ipynb']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "path=\"/content/drive/MyDrive/Dataset/Dataset.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(path, nrows = 3000)\n",
        "    print(\"Data loaded successfully!\")\n",
        "\n",
        "    # Step 5: Display the first 5 rows of the DataFrame\n",
        "    print(df.head())\n",
        "\n",
        "    print(f\"Number of rows: {df.shape[0]}, Number of columns: {df.shape[1]}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the path and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PaF41mYY_VE",
        "outputId": "a9875e47-43f6-4f78-9b9d-73e2dfa38eeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n",
            "   Vehicle ID  Frame ID  Total Frames   Global Time  Local X  Local Y  \\\n",
            "0           2        14           437  1.120000e+12   16.447   39.381   \n",
            "1           2        15           437  1.120000e+12   16.426   43.381   \n",
            "2           2        16           437  1.120000e+12   16.405   47.380   \n",
            "3           2        17           437  1.120000e+12   16.385   51.381   \n",
            "4           2        18           437  1.120000e+12   16.364   55.381   \n",
            "\n",
            "      Global X     Global Y  Vehicle Length  Vehicle Width  Vehicle Class  \\\n",
            "0  6451140.329  1873342.000            14.5            4.9              2   \n",
            "1  6451143.018  1873339.038            14.5            4.9              2   \n",
            "2  6451145.706  1873336.077            14.5            4.9              2   \n",
            "3  6451148.395  1873333.115            14.5            4.9              2   \n",
            "4  6451151.084  1873330.153            14.5            4.9              2   \n",
            "\n",
            "   Vehicle Velocity  Vehicle Accl  Lane Identification  Preceeding Vehicle  \\\n",
            "0              40.0           0.0                    2                   0   \n",
            "1              40.0           0.0                    2                   0   \n",
            "2              40.0           0.0                    2                   0   \n",
            "3              40.0           0.0                    2                   0   \n",
            "4              40.0           0.0                    2                   0   \n",
            "\n",
            "   Following Vehicle  Spacing  Headway  \n",
            "0                  0      0.0      0.0  \n",
            "1                  0      0.0      0.0  \n",
            "2                  0      0.0      0.0  \n",
            "3                  0      0.0      0.0  \n",
            "4                  0      0.0      0.0  \n",
            "Number of rows: 3000, Number of columns: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the nodes\n",
        "node_features = df[['Local X', 'Local Y', 'Vehicle Velocity', 'Vehicle Accl']]\n",
        "node_features = (node_features - node_features.mean()) / node_features.std()\n",
        "node_features_tensor = torch.tensor(node_features.values, dtype=torch.float)"
      ],
      "metadata": {
        "id": "f-ZxPf0ELCHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst 5 Node Features:\")\n",
        "print(node_features_tensor[:5])  # Change 5 to any number to see more/less nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJO0oGzTzMwl",
        "outputId": "30f0c6b3-1b5c-432d-ac42-e2739d72167a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 Node Features:\n",
            "tensor([[-0.8727, -1.6897, -0.9491, -0.0810],\n",
            "        [-0.8738, -1.6830, -0.9491, -0.0810],\n",
            "        [-0.8749, -1.6762, -0.9491, -0.0810],\n",
            "        [-0.8760, -1.6694, -0.9491, -0.0810],\n",
            "        [-0.8771, -1.6626, -0.9491, -0.0810]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create edges based on Preceding/Following Vehicle relationships and a proximity radius\n",
        "def create_edges_with_proximity(df, radius):\n",
        "    edge_index = []\n",
        "\n",
        "    # Iterate through the dataframe to establish edges between vehicles\n",
        "    for i, row in df.iterrows():\n",
        "        preceding_vehicle = row['Preceeding Vehicle']\n",
        "        following_vehicle = row['Following Vehicle']\n",
        "\n",
        "        # Helper function to check distance and create edge\n",
        "        def add_edge_if_within_radius(i, vehicle_id):\n",
        "            if vehicle_id != 0:\n",
        "                vehicle_index = df[df['Vehicle ID'] == vehicle_id].index\n",
        "                if len(vehicle_index) > 0:\n",
        "                    vehicle_index = vehicle_index[0]\n",
        "\n",
        "                    # Calculate the Euclidean distance between the vehicles\n",
        "                    distance = np.sqrt((row['Local X'] - df.loc[vehicle_index]['Local X'])**2 +\n",
        "                                       (row['Local Y'] - df.loc[vehicle_index]['Local Y'])**2)\n",
        "\n",
        "                    # If within the radius, create bidirectional edges\n",
        "                    if distance <= radius:\n",
        "                        edge_index.append([i, vehicle_index])\n",
        "                        edge_index.append([vehicle_index, i])  # Add both directions for an undirected graph\n",
        "\n",
        "        # Check and add edges for the preceding vehicle\n",
        "        add_edge_if_within_radius(i, preceding_vehicle)\n",
        "\n",
        "        # Check and add edges for the following vehicle\n",
        "        add_edge_if_within_radius(i, following_vehicle)\n",
        "\n",
        "    # Convert edge_index to tensor and transpose it to (2, num_edges)\n",
        "    return torch.tensor(edge_index, dtype=torch.long).t()\n",
        "\n",
        "# Define a proximity radius (e.g., 100 feet)\n",
        "proximity_radius = 100\n",
        "\n",
        "# Step 4: Create edge index (edges) based on Preceding/Following Vehicle and proximity\n",
        "edge_index = create_edges_with_proximity(df, proximity_radius)\n",
        "\n",
        "# Step 5: Create a PyTorch Geometric Data object\n",
        "data = Data(x=node_features_tensor, edge_index=edge_index)\n",
        "\n",
        "# Step 6: Check the Data object\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iXgIb1Vzb4G",
        "outputId": "f027b45d-0fab-435e-922d-c8454e5e3a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[3000, 4], edge_index=[2, 180])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GATNet Module\n",
        "class GATNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads):\n",
        "        super(GATNet, self).__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=num_heads, concat=True, dropout=0.2)\n",
        "        self.gat2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, concat=True, dropout=0.2)\n",
        "        self.gat3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, concat=True, dropout=0.2)\n",
        "        self.gat4 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, concat=False)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)  # Match output_dim with Transformer input_dim\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.elu(self.gat1(x, edge_index))\n",
        "        x = F.elu(self.gat2(x, edge_index))\n",
        "        x = F.elu(self.gat3(x, edge_index))\n",
        "        x = F.elu(self.gat4(x, edge_index))\n",
        "        x = self.fc(x)  # Final output dimension should match Transformer’s input_dim\n",
        "        return x"
      ],
      "metadata": {
        "id": "Xq4Zhho09xNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Model Hyperparameters\n",
        "input_dim = 4  # Number of node features (Local X, Local Y, Vehicle Velocity, Vehicle Accl)\n",
        "hidden_dim = 32  # Number of hidden units in GAT layers\n",
        "output_dim = 2  # Predicting 2D coordinates (x, y)\n",
        "num_heads = 4   # Number of attention heads"
      ],
      "metadata": {
        "id": "BmO-IVZD5uWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = GATNet(input_dim, hidden_dim, output_dim, num_heads)"
      ],
      "metadata": {
        "id": "gntLJ3BP55p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Optimizer and Loss Function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "loss_fn = torch.nn.MSELoss()  # Using Mean Squared Error for regression (predicting coordinates)"
      ],
      "metadata": {
        "id": "ZVmyapz258_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Add the next positions (Next X, Next Y) as the target labels\n",
        "df['Next X'] = df.groupby('Vehicle ID')['Local X'].shift(-1)\n",
        "df['Next Y'] = df.groupby('Vehicle ID')['Local Y'].shift(-1)\n",
        "\n",
        "# Remove NaN values (last row of each vehicle will not have a next position)\n",
        "df = df.dropna(subset=['Next X', 'Next Y'])\n",
        "\n",
        "# Standardize node features (if not done previously)\n",
        "scaler_features = StandardScaler()\n",
        "node_features = scaler_features.fit_transform(df[['Local X', 'Local Y', 'Vehicle Velocity', 'Vehicle Accl']].values)\n",
        "node_features_tensor = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "# Standardize target values for Next X and Next Y\n",
        "scaler_target = StandardScaler()\n",
        "target = scaler_target.fit_transform(df[['Next X', 'Next Y']].values)\n",
        "target_tensor = torch.tensor(target, dtype=torch.float)\n",
        "\n",
        "# Create the PyTorch Geometric Data object with the standardized targets\n",
        "data = Data(x=node_features_tensor, edge_index=edge_index, y=target_tensor)"
      ],
      "metadata": {
        "id": "_mbPpniQEapQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the node features and targets into training and testing sets\n",
        "train_mask, test_mask = train_test_split(np.arange(len(df)), test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Reindex edge_index for training data ---\n",
        "# 1. Filter edges based on train_mask (instead of indexing directly)\n",
        "train_edge_index = edge_index[:, np.isin(edge_index[0], train_mask) & np.isin(edge_index[1], train_mask)]\n",
        "\n",
        "# 2. Get unique node indices in the filtered training set\n",
        "train_nodes = np.unique(train_edge_index)\n",
        "\n",
        "# 3. Create a mapping from original node indices to new indices in the training set\n",
        "node_map = {node: i for i, node in enumerate(train_nodes)}\n",
        "\n",
        "# 4. Reindex train_edge_index using the mapping\n",
        "train_edge_index = torch.tensor([[node_map[n1], node_map[n2]]\n",
        "                                  for n1, n2 in train_edge_index.t().tolist()], dtype=torch.long).t()\n",
        "\n",
        "# --- Repeat the same process for test data ---\n",
        "# 1. Filter edges based on test_mask\n",
        "test_edge_index = edge_index[:, np.isin(edge_index[0], test_mask) & np.isin(edge_index[1], test_mask)]\n",
        "\n",
        "# 2. Get unique node indices in the filtered test set\n",
        "test_nodes = np.unique(test_edge_index)\n",
        "\n",
        "# 3. Create a mapping from original node indices to new indices in the test set\n",
        "node_map = {node: i for i, node in enumerate(test_nodes)}\n",
        "\n",
        "# 4. Reindex test_edge_index using the mapping\n",
        "test_edge_index = torch.tensor([[node_map[n1], node_map[n2]]\n",
        "                                 for n1, n2 in test_edge_index.t().tolist()], dtype=torch.long).t()\n",
        "\n",
        "\n",
        "train_data = Data(x=node_features_tensor[train_mask],\n",
        "                  edge_index=train_edge_index,  # Use reindexed edge_index\n",
        "                  y=target_tensor[train_mask])\n",
        "\n",
        "test_data = Data(x=node_features_tensor[test_mask],\n",
        "                 edge_index=test_edge_index,   # Use reindexed edge_index\n",
        "                 y=target_tensor[test_mask])"
      ],
      "metadata": {
        "id": "eriuxYLUE2i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Define a training loop\n",
        "def train_model(model, data, epochs=100):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        # Forward pass through the model\n",
        "        out = model(data)\n",
        "\n",
        "        # Assuming the ground truth for trajectory is in the Data object (use actual labels in practice)\n",
        "        target = data.y  # Placeholder for ground truth labels\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(out, target)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print loss at every epoch\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n"
      ],
      "metadata": {
        "id": "RMFcqhED6Bml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_data):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation for faster testing\n",
        "        out = model(test_data)  # Make predictions on the test data\n",
        "\n",
        "        # Compute the test loss (MSE)\n",
        "        test_loss = loss_fn(out, test_data.y)\n",
        "        print(f'Test Loss (MSE): {test_loss.item()}')\n",
        "\n",
        "        # Convert predictions and actual values to numpy arrays\n",
        "        predicted = out.numpy()\n",
        "        actual = test_data.y.numpy()\n",
        "\n",
        "        # Calculate ADE (Average Displacement Error)\n",
        "        # ADE is the mean Euclidean distance between each predicted and actual trajectory point\n",
        "        displacement_errors = np.linalg.norm(predicted - actual, axis=1)\n",
        "        ade = np.mean(displacement_errors)\n",
        "        print(f'Average Displacement Error (ADE): {ade}')\n",
        "\n",
        "        # Calculate RMSE (Root Mean Squared Error)\n",
        "        rmse = np.sqrt(np.mean((predicted - actual) ** 2))\n",
        "        print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "\n",
        "        # Display predicted vs actual for the first 5 examples\n",
        "        print(\"\\nPredicted vs Actual (First 5 examples):\")\n",
        "        print(\"Predicted:\", predicted[:5])\n",
        "        print(\"Actual:\", actual[:5])"
      ],
      "metadata": {
        "id": "ZfstDte26J8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, train the model\n",
        "train_model(model, train_data, epochs=100)\n",
        "\n",
        "# After training, evaluate the model on the test set\n",
        "test_model(model, test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6nKgFYmFljK",
        "outputId": "fa93aabb-a6f6-449e-c8bd-4da03b5e33c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0328220129013062\n",
            "Epoch 10, Loss: 0.7820576429367065\n",
            "Epoch 20, Loss: 0.5778877139091492\n",
            "Epoch 30, Loss: 0.4358111023902893\n",
            "Epoch 40, Loss: 0.33288997411727905\n",
            "Epoch 50, Loss: 0.2761146128177643\n",
            "Epoch 60, Loss: 0.22387659549713135\n",
            "Epoch 70, Loss: 0.1968245655298233\n",
            "Epoch 80, Loss: 0.187203511595726\n",
            "Epoch 90, Loss: 0.18499930202960968\n",
            "Test Loss (MSE): 0.04063253477215767\n",
            "Average Displacement Error (ADE): 0.21410979330539703\n",
            "Root Mean Squared Error (RMSE): 0.2015751302242279\n",
            "\n",
            "Predicted vs Actual (First 5 examples):\n",
            "Predicted: [[-1.1486914   0.03611059]\n",
            " [-1.1166267  -0.25966853]\n",
            " [-0.99179137 -0.27861202]\n",
            " [ 0.31421944 -0.32646197]\n",
            " [ 0.41364947 -0.3089751 ]]\n",
            "Actual: [[-1.2866697   1.5843787 ]\n",
            " [-1.4149683  -0.00394412]\n",
            " [ 1.2027675   0.01464173]\n",
            " [-0.90751415 -1.3513286 ]\n",
            " [ 0.22017324 -1.5468384 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Transformer Module*"
      ],
      "metadata": {
        "id": "xSWH4RZF5mXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "\n",
        "# Define the DecoderOnlyTransformer Module\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, seq_len, input_dim, embed_dim, num_heads, ff_dim, num_layers, output_dim):\n",
        "        super(DecoderOnlyTransformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embedding = nn.Linear(input_dim, embed_dim)  # Embedding layer to match GATNet output to embed_dim\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_len, embed_dim))  # Positional encoding\n",
        "\n",
        "        # Transformer decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Linear layer to map the final output to desired output_dim (2 for (x, y))\n",
        "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed GATNet output and add positional encoding\n",
        "        embed_dim=32\n",
        "        seq_len=10\n",
        "        device = x.device\n",
        "        #self.embedding = nn.Linear(input_dim, embed_dim)\n",
        "        # Change here: seq_len should be num_nodes\n",
        "        #self.positional_encoding = nn.Parameter(torch.randn(1, seq_len, embed_dim))\n",
        "        self.embedding = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        batch_size = x.shape[0]  # Get batch size from input\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, self.embed_dim, self.embed_dim), requires_grad=True).to(device)\n",
        "        positional_encoding = self.positional_encoding.repeat(batch_size, 1, 1)  # Repeat for batch size\n",
        "        #self.positional_encoding = nn.Parameter(torch.randn(1, x.shape[1], self.embed_dim)).to(x.device)\n",
        "\n",
        "        #x = self.embedding(x) + self.positional_encoding\n",
        "        x = self.embedding(x) + positional_encoding[:, :x.shape[1], :]\n",
        "\n",
        "        # Pass through each decoder layer\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, x)  # Both tgt and memory are x for decoder-only\n",
        "\n",
        "        # Project to the output dimension (e.g., 2 for (x, y) positions)\n",
        "        output = self.fc_out(x)\n",
        "        return output\n",
        "\n",
        "# Sample Execution Code\n",
        "\n",
        "# Instantiate the GAT and Transformer models\n",
        "gat_model = GATNet(input_dim=4, hidden_dim=32, output_dim=32, num_heads=4)  # output_dim = 32 to match Transformer’s embed_dim\n",
        "transformer_model = DecoderOnlyTransformer(seq_len=10, input_dim=32, embed_dim=32, num_heads=4, ff_dim=128, num_layers=8, output_dim=2)\n",
        "\n",
        "# Sample data for GATNet\n",
        "from torch_geometric.data import Data\n",
        "input_dim = 4\n",
        "num_nodes = 32\n",
        "x = torch.rand((num_nodes, input_dim))  # Random node features\n",
        "edge_index = torch.randint(0, num_nodes, (2, 150))  # Random edges for the graph\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Run GATNet to obtain embeddings\n",
        "gat_output = gat_model(data)  # Shape: (num_nodes, 32) - (32 is the embed_dim for Transformer)\n",
        "\n",
        "# Reshape GATNet output to match Transformer input\n",
        "gat_output = gat_output.unsqueeze(0)  # Add batch dimension, shape: (1, num_nodes, 32)\n",
        "\n",
        "# Run Transformer on GATNet output\n",
        "output = transformer_model(gat_output)\n",
        "print(\"Output shape:\", output.shape)  # Expected shape: (1, num_nodes, 2)\n"
      ],
      "metadata": {
        "id": "qVqAspxI95R2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07f56d8-477e-4cea-b9e2-32973cefa691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 32, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Load and Preprocess the Dataset*"
      ],
      "metadata": {
        "id": "iymcnMNY4-Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads):\n",
        "        super(DecoderOnlyTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(hidden_dim, num_heads),\n",
        "            num_layers\n",
        "        )\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.positional_encoding = self.generate_positional_encoding(hidden_dim, 5000)  # Increased max_len\n",
        "\n",
        "\n",
        "    def generate_positional_encoding(self, d_model, max_len):\n",
        "        \"\"\"\n",
        "        Generate positional encoding for Transformer.\n",
        "\n",
        "        Args:\n",
        "            d_model: Dimension of the model.\n",
        "            max_len: Maximum sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Positional encoding tensor of shape (max_len, d_model).\n",
        "        \"\"\"\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the DecoderOnlyTransformer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, sequence_length, output_dim).\n",
        "        \"\"\"\n",
        "        # Reshape before linear layer\n",
        "        x = x.reshape(-1, self.embedding.in_features)\n",
        "\n",
        "        x = self.embedding(x) + self.positional_encoding[:, :x.shape[0], :]\n",
        "\n",
        "        # Reshape for TransformerDecoder\n",
        "        batch_size = x.shape[0] // num_nodes  # Assuming num_nodes is a global variable\n",
        "        x = x.view(num_nodes, batch_size, -1)  # (sequence_length, batch_size, embedding_dim)\n",
        "\n",
        "        # TransformerDecoder expects (sequence_length, batch_size, embedding_dim)\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Reshape back to (batch_size * num_nodes, output_dim)\n",
        "        x = x.view(batch_size * num_nodes, -1)\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "'''"
      ],
      "metadata": {
        "id": "hNggnWXLChy1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "711be3c6-ae29-4a59-b42a-1b266d49da15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nimport torch.nn as nn\\n\\nclass DecoderOnlyTransformer(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads):\\n        super(DecoderOnlyTransformer, self).__init__()\\n        self.embedding = nn.Linear(input_dim, hidden_dim)\\n        self.transformer = nn.TransformerDecoder(\\n            nn.TransformerDecoderLayer(hidden_dim, num_heads),\\n            num_layers\\n        )\\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\\n        self.positional_encoding = self.generate_positional_encoding(hidden_dim, 5000)  # Increased max_len\\n\\n\\n    def generate_positional_encoding(self, d_model, max_len):\\n        \"\"\"\\n        Generate positional encoding for Transformer.\\n\\n        Args:\\n            d_model: Dimension of the model.\\n            max_len: Maximum sequence length.\\n\\n        Returns:\\n            Positional encoding tensor of shape (max_len, d_model).\\n        \"\"\"\\n        pe = torch.zeros(max_len, d_model)\\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\\n        pe[:, 0::2] = torch.sin(position * div_term)\\n        pe[:, 1::2] = torch.cos(position * div_term)\\n        pe = pe.unsqueeze(0)  # Add batch dimension\\n        return pe\\n\\n    def forward(self, x):\\n        \"\"\"\\n        Forward pass of the DecoderOnlyTransformer.\\n\\n        Args:\\n            x: Input tensor of shape (batch_size, sequence_length, input_dim).\\n\\n        Returns:\\n            Output tensor of shape (batch_size, sequence_length, output_dim).\\n        \"\"\"\\n        # Reshape before linear layer\\n        x = x.reshape(-1, self.embedding.in_features)\\n\\n        x = self.embedding(x) + self.positional_encoding[:, :x.shape[0], :]\\n\\n        # Reshape for TransformerDecoder\\n        batch_size = x.shape[0] // num_nodes  # Assuming num_nodes is a global variable\\n        x = x.view(num_nodes, batch_size, -1)  # (sequence_length, batch_size, embedding_dim)\\n\\n        # TransformerDecoder expects (sequence_length, batch_size, embedding_dim)\\n        x = self.transformer(x)\\n\\n        # Reshape back to (batch_size * num_nodes, output_dim)\\n        x = x.view(batch_size * num_nodes, -1)\\n        x = self.fc_out(x)\\n        return x\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load dataset\n",
        "data_path = path\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "# Take a random sample of 3000 rows\n",
        "sampled_data = data.sample(n=3000, random_state=42)  # random_state for reproducibility\n",
        "\n",
        "sampled_data['Next X'] = sampled_data.groupby('Vehicle ID')['Local X'].shift(-1)\n",
        "sampled_data['Next Y'] = sampled_data.groupby('Vehicle ID')['Local Y'].shift(-1)\n",
        "sampled_data = sampled_data.dropna(subset=['Next X', 'Next Y'])\n",
        "\n",
        "# Preprocess data\n",
        "input_columns = ['Local X', 'Local Y', 'Vehicle Velocity', 'Vehicle Accl']\n",
        "target_columns = ['Next X','Next Y']\n",
        "\n",
        "all_columns = input_columns + target_columns\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "sampled_data[all_columns] = scaler.fit_transform(sampled_data[all_columns])\n",
        "'''\n",
        "sampled_data[input_columns] = scaler.fit_transform(sampled_data[input_columns])\n",
        "sampled_data[target_columns] = scaler.transform(sampled_data[target_columns])\n",
        "'''\n",
        "\n",
        "# Convert to torch tensors\n",
        "X = torch.tensor(sampled_data[input_columns].values, dtype=torch.float32) #input\n",
        "y = torch.tensor(sampled_data[target_columns].values, dtype=torch.float32) #output\n",
        "\n",
        "# Reshape data to fit (samples, sequence length, input/output dim)\n",
        "seq_len = 10\n",
        "num_samples = X.shape[0] // seq_len\n",
        "X = X[:num_samples * seq_len].view(num_samples, seq_len, -1)  # Shape: (samples, seq_len, input_dim)\n",
        "y = y[:num_samples * seq_len].view(num_samples, seq_len, -1)  # Shape: (samples, seq_len, output_dim)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 8\n",
        "train_data = TensorDataset(X, y)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "AO1YxXYM1M6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X contains normalized inputs and y contains normalized targets\n",
        "\n",
        "# Input data statistics\n",
        "print(\"Input Data Statistics:\")\n",
        "print(\"Min:\", X.min().item(), \"Max:\", X.max().item())\n",
        "print(\"Mean:\", X.mean().item(), \"Std Dev:\", X.std().item())\n",
        "\n",
        "# Target data statistics\n",
        "print(\"\\nTarget Data Statistics:\")\n",
        "print(\"Min:\", y.min().item(), \"Max:\", y.max().item())\n",
        "print(\"Mean:\", y.mean().item(), \"Std Dev:\", y.std().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFNtmDYw-UMm",
        "outputId": "13de1f45-ea18-4de0-942a-2aae16c1a325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data Statistics:\n",
            "Min: 0.0 Max: 1.0\n",
            "Mean: 0.46156561374664307 Std Dev: 0.24507516622543335\n",
            "\n",
            "Target Data Statistics:\n",
            "Min: 0.0 Max: 1.0\n",
            "Mean: 0.41317644715309143 Std Dev: 0.27452149987220764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*split the data form training,testing and validation*"
      ],
      "metadata": {
        "id": "vhk8dGeH6ukb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Define the split sizes (e.g., 60% train, 20% validation, 20% test)\n",
        "train_size = int(0.6 * len(train_data))\n",
        "val_size = int(0.2 * len(train_data))\n",
        "test_size = len(train_data) - train_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(train_data, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "'''"
      ],
      "metadata": {
        "id": "-HxxmqwS6tcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "01be76d7-75a0-4bbc-a6e6-4f10c633864e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Define the split sizes (e.g., 60% train, 20% validation, 20% test)\\ntrain_size = int(0.6 * len(train_data))\\nval_size = int(0.2 * len(train_data))\\ntest_size = len(train_data) - train_size - val_size\\n\\n# Split the dataset\\ntrain_dataset, val_dataset, test_dataset = random_split(train_data, [train_size, val_size, test_size])\\n\\n# Create DataLoaders for each set\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Training Loop*"
      ],
      "metadata": {
        "id": "x9E-mXdS5AlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data_list in train_loader:\n",
        "    print([type(item) for item in data_list])  # Check types in data_list\n",
        "    break  # Check the first batch only"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clMNqHjl6IjJ",
        "outputId": "3db0e85e-589f-4102-dee2-64c04fe4a029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<class 'torch.Tensor'>, <class 'torch.Tensor'>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "num_edges=150\n",
        "# Example tensors for a single graph sample\n",
        "x = torch.rand((num_nodes, input_dim))  # Node features tensor\n",
        "edge_index = torch.randint(0, num_nodes, (2, num_edges))  # Edge indices tensor\n",
        "y = torch.rand((num_nodes, output_dim))  # Target values tensor\n",
        "\n",
        "# Create a PyTorch Geometric Data object\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n"
      ],
      "metadata": {
        "id": "Rwktefhd7kkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Assume `data_list` contains multiple `Data` objects, one for each graph\n",
        "data_list = [Data(x=torch.rand((num_nodes, input_dim)),\n",
        "                  edge_index=torch.randint(0, num_nodes, (2, num_edges)),\n",
        "                  y=torch.rand((num_nodes, output_dim))) for _ in range(num_samples)]\n",
        "'''"
      ],
      "metadata": {
        "id": "uwRRRjry8L0H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "4e6aa302-87ec-4d4f-8ee6-eb7549b1d038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Assume `data_list` contains multiple `Data` objects, one for each graph\\ndata_list = [Data(x=torch.rand((num_nodes, input_dim)),\\n                  edge_index=torch.randint(0, num_nodes, (2, num_edges)),\\n                  y=torch.rand((num_nodes, output_dim))) for _ in range(num_samples)]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "# Create a DataLoader with batching enabled\n",
        "\n",
        "train_size = int(0.6 * len(train_data))\n",
        "val_size = int(0.2 * len(train_data))\n",
        "test_size = len(train_data) - train_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(train_data, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders for each set\n",
        "train_loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(data_list, batch_size=32, shuffle=False)\n",
        "'''"
      ],
      "metadata": {
        "id": "T2WiZDYn8Q0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "c480590c-5652-4ec0-eefe-f1feb2d67239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom torch_geometric.data import DataLoader\\n\\n# Create a DataLoader with batching enabled\\n\\ntrain_size = int(0.6 * len(train_data))\\nval_size = int(0.2 * len(train_data))\\ntest_size = len(train_data) - train_size - val_size\\n\\n# Split the dataset\\ntrain_dataset, val_dataset, test_dataset = random_split(train_data, [train_size, val_size, test_size])\\n\\n# Create DataLoaders for each set\\ntrain_loader = DataLoader(data_list, batch_size=32, shuffle=True)\\nval_loader = DataLoader(data_list, batch_size=32, shuffle=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data, DataLoader,Batch\n",
        "\n",
        "# Example data creation\n",
        "num_nodes = 32\n",
        "input_dim = 4\n",
        "output_dim = 2\n",
        "num_edges =150\n",
        "\n",
        "# Generate some example graphs\n",
        "data_list = [\n",
        "    Data(\n",
        "        x=torch.rand((num_nodes, input_dim)),           # Node features\n",
        "        edge_index=torch.randint(0, num_nodes, (2, num_edges)),  # Edge indices\n",
        "        y=torch.rand((num_nodes, output_dim))            # Target values\n",
        "    )\n",
        "    for _ in range(100)  # Assume 100 samples\n",
        "]\n",
        "\n",
        "# Use this list of Data objects to create DataLoaders\n",
        "train_loader = DataLoader(data_list, batch_size=32, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(data_list, batch_size=32, shuffle=False, drop_last=True)\n",
        "\n",
        "'''\n",
        "train_loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(data_list, batch_size=32, shuffle=False)\n",
        "'''\n"
      ],
      "metadata": {
        "id": "6sjSsAP3GUGy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "71991d27-108a-4a16-8d54-84ca0f7c947e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrain_loader = DataLoader(data_list, batch_size=32, shuffle=True)\\nval_loader = DataLoader(data_list, batch_size=32, shuffle=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'*DEBUGGING*"
      ],
      "metadata": {
        "id": "ujX4fXLeITfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for data_list in train_loader:\n",
        "    # Verify that `data_list` is a list of `Data` objects\n",
        "    print(f\"Type of data_list: {type(data_list)}\")\n",
        "    print(f\"Type of first item in data_list: {type(data_list[0])}\")\n",
        "\n",
        "    # Try to combine into a Batch and check the result\n",
        "    try:\n",
        "        data = Batch.from_data_list(data_list).to(device)\n",
        "        print(f\"Batched data type: {type(data)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during batching: {e}\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "H0j5v_5VHwsr",
        "outputId": "0da0bd6b-af37-4278-ffc1-2861995ee7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor data_list in train_loader:\\n    # Verify that `data_list` is a list of `Data` objects\\n    print(f\"Type of data_list: {type(data_list)}\")\\n    print(f\"Type of first item in data_list: {type(data_list[0])}\")\\n\\n    # Try to combine into a Batch and check the result\\n    try:\\n        data = Batch.from_data_list(data_list).to(device)\\n        print(f\"Batched data type: {type(data)}\")\\n    except Exception as e:\\n        print(f\"Error during batching: {e}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for data_list in train_loader:\n",
        "    # Check the type of the batch and its contents\n",
        "    print(f\"Batch type: {type(data_list)}\")          # Should print <class 'list'>\n",
        "    print(f\"First item type in batch: {type(data_list[0])}\")  # Should print <class 'torch_geometric.data.data.Data'>\n",
        "\n",
        "    # If any unexpected types appear, this is where they might be introduced.\n",
        "    break  # Check only the first batch to reduce output\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "HL_c8XqlIXg6",
        "outputId": "149e9b55-5015-4a0e-ba10-316ee816a294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor data_list in train_loader:\\n    # Check the type of the batch and its contents\\n    print(f\"Batch type: {type(data_list)}\")          # Should print <class \\'list\\'>\\n    print(f\"First item type in batch: {type(data_list[0])}\")  # Should print <class \\'torch_geometric.data.data.Data\\'>\\n\\n    # If any unexpected types appear, this is where they might be introduced.\\n    break  # Check only the first batch to reduce output\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for data in train_loader:  # `data` is already a batched Data object\n",
        "    data = data.to(device)  # Move the entire batch to the device\n",
        "\n",
        "    # Run the inputs through GATNet\n",
        "    gat_output = gat_model(data)  # `data` contains `x`, `edge_index`, etc.\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8QDvu55bMK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "9d38ae33-a94f-44d3-eef9-b04728f911ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\nfor data in train_loader:  # `data` is already a batched Data object\\n    data = data.to(device)  # Move the entire batch to the device\\n\\n    # Run the inputs through GATNet\\n    gat_output = gat_model(data)  # `data` contains `x`, `edge_index`, etc.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TRAINING LOOP*"
      ],
      "metadata": {
        "id": "rB-Cx814IoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import DataLoader, Batch\n",
        "\n",
        "# Define models, optimizer, etc.\n",
        "optimizer = optim.Adam(list(gat_model.parameters()) + list(transformer_model.parameters()), lr=0.0001)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "for data in train_loader:  # `data` is already a batched Data object\n",
        "    data = data.to(device)  # Move the batch to the device\n",
        "\n",
        "    # Step 1: Run the inputs through GATNet\n",
        "    gat_output = gat_model(data)  # Expects `data.x` and `data.edge_index`\n",
        "\n",
        "    # Step 2: Reshape or process `gat_output` as needed\n",
        "    # Assuming you need to reshape it for the Transformer input\n",
        "    batch_size = data.num_graphs\n",
        "    sequence_length = gat_output.size(0) // batch_size\n",
        "    gat_output = gat_output.view(batch_size, sequence_length, -1)\n",
        "\n",
        "    # Step 3: Run through Transformer\n",
        "    outputs = transformer_model(gat_output)\n",
        "\n",
        "    # Step 4: Compute loss\n",
        "    targets = data.y.to(device)  # Assuming `data.y` is the target\n",
        "    targets = targets.view(outputs.shape)  # Reshape targets to match outputs if needed\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    '''\n"
      ],
      "metadata": {
        "id": "VYhcj55D5pPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(list(gat_model.parameters()) + list(transformer_model.parameters()), lr=0.0001)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Send models to the device\n",
        "gat_model.to(device)\n",
        "transformer_model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Set models to training mode\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")   # Should output <class 'torch_geometric.data.Data'>\n",
        "\n",
        "    #Combine list of Data objects into a single Batch object\n",
        "    #data = Batch.from_data_list(data_list).to(device)\n",
        "    #data = data_list.to(device)\n",
        "    gat_model.train()\n",
        "    transformer_model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for data in train_loader:  # Iterate through batches (DataBatch objects) directly\n",
        "        # Move data to the device\n",
        "        data = data.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        # Step 1: Run the inputs through GATNet\n",
        "        gat_output = gat_model(data)  # Expects `data.x` and `data.edge_index` in GATNet\n",
        "\n",
        "        # Check the shape of gat_output\n",
        "        print(f\"GAT output shape: {gat_output.shape}\")\n",
        "\n",
        "        # Step 2: Reshape GAT output if needed\n",
        "        # Ensure gat_output has shape [batch_size, seq_len (num_nodes), embed_dim]\n",
        "        if len(gat_output.shape) == 2:  # If it’s [num_nodes * batch_size, embed_dim]\n",
        "            batch_size = data.num_graphs # Get number of graphs in the batch #changed to data.num_graphs\n",
        "            num_nodes = data.x.shape[0] // batch_size  # Compute nodes per graph\n",
        "            gat_output = gat_output.view(batch_size, num_nodes, -1)\n",
        "        # Check the reshaped shape\n",
        "        print(f\"Reshaped GAT output shape: {gat_output.shape}\")\n",
        "\n",
        "        # Step 3: Run the GAT output through Transformer directly\n",
        "        outputs = transformer_model(gat_output)\n",
        "\n",
        "        # Step 4: Compute loss between model output and targets\n",
        "        targets = data.y.to(device)  # Assuming targets are stored as `data.y`\n",
        "\n",
        "        # Check shapes of outputs and targets\n",
        "        print(f\"Outputs shape: {outputs.shape}\")\n",
        "        print(f\"Targets shape: {targets.shape}\")\n",
        "\n",
        "        targets = targets.view(outputs.shape)  # Reshape targets to match outputs shape\n",
        "\n",
        "        # Ensure targets have the same shape as outputs\n",
        "        if targets.shape != outputs.shape:\n",
        "            raise ValueError(f\"Targets shape {targets.shape} does not match Outputs shape {outputs.shape}\")\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss per epoch\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\n",
        "    # Validation phase\n",
        "    gat_model.eval()\n",
        "    transformer_model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # Disable gradients for validation\n",
        "        for data_list in val_loader:  # Expect each batch to be a list of Data objects\n",
        "            # Combine list of Data objects into a single Batch object\n",
        "            #data = Batch.from_data_list(data_list).to(device)\n",
        "            data = data_list.to(device)\n",
        "\n",
        "            # Run inputs through GAT and then Transformer\n",
        "            gat_output = gat_model(data)\n",
        "\n",
        "            # Reshape if needed\n",
        "            if len(gat_output.shape) == 2:\n",
        "                batch_size = data.num_graphs\n",
        "                num_nodes = data.x.shape[0] // batch_size\n",
        "                gat_output = gat_output.view(batch_size, num_nodes, -1)\n",
        "\n",
        "            # Get predictions from Transformer\n",
        "            targets = data.y.to(device)  # Assuming targets are stored as `data.y`\n",
        "            outputs = transformer_model(gat_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDR-XRkZ1oSc",
        "outputId": "d0979b50-22be-4267-e951-956c49099381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [1/10], Training Loss: 0.0964\n",
            "Epoch [2/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [2/10], Training Loss: 0.0955\n",
            "Epoch [3/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [3/10], Training Loss: 0.0942\n",
            "Epoch [4/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [4/10], Training Loss: 0.0941\n",
            "Epoch [5/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [5/10], Training Loss: 0.0937\n",
            "Epoch [6/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [6/10], Training Loss: 0.0926\n",
            "Epoch [7/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [7/10], Training Loss: 0.0925\n",
            "Epoch [8/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [8/10], Training Loss: 0.0922\n",
            "Epoch [9/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [9/10], Training Loss: 0.0921\n",
            "Epoch [10/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [10/10], Training Loss: 0.0913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()  # Loss function for regression\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50  # Adjust number of epochs based on model performance\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, targets)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)  # Average loss for the epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # No gradients needed during validation\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "'''"
      ],
      "metadata": {
        "id": "hhDp1PFL77Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Testing*"
      ],
      "metadata": {
        "id": "xc1DzkjAAb1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set the models to evaluation mode\n",
        "gat_model.eval()\n",
        "transformer_model.eval()\n",
        "\n",
        "# Placeholder to accumulate test loss\n",
        "test_loss = 0.0\n",
        "loss_fn = nn.MSELoss()  # Define the loss function (MSE for regression tasks)\n",
        "\n",
        "# Initialize lists to store predicted and actual values\n",
        "predicted_values = []\n",
        "actual_values = []\n",
        "\n",
        "# Evaluation loop\n",
        "with torch.no_grad():  # Disable gradient computation for faster testing\n",
        "    for data in val_loader:  # Assuming `val_loader` yields `Data` objects from PyTorch Geometric\n",
        "        data = data.to(device)\n",
        "\n",
        "        # Forward pass: Get predictions through GATNet and Transformer\n",
        "        gat_output = gat_model(data)  # Process input through GATNet\n",
        "        batch_size = data.num_graphs\n",
        "        seq_len = gat_output.size(0) // batch_size\n",
        "        gat_output = gat_output.view(batch_size, seq_len, -1)  # Reshape as needed\n",
        "        outputs = transformer_model(gat_output)  # Get predictions from Transformer\n",
        "\n",
        "        # Get targets\n",
        "        targets = data.y.to(device)\n",
        "        targets = targets.view(outputs.shape)  # Reshape targets to match outputs\n",
        "\n",
        "        # Compute test loss\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Store predictions and actual targets (convert tensors to numpy arrays)\n",
        "        predicted_values.append(outputs.cpu().numpy())  # Move to CPU and convert to numpy\n",
        "        actual_values.append(targets.cpu().numpy())  # Move to CPU and convert to numpy\n",
        "\n",
        "# Calculate the average test loss\n",
        "avg_test_loss = test_loss / len(val_loader)\n",
        "print(f\"Test Loss (MSE): {avg_test_loss:.4f}\")\n",
        "\n",
        "# Concatenate all predicted and actual values for further metrics\n",
        "predicted_values = np.concatenate(predicted_values, axis=0)  # Shape: (num_samples, seq_len, output_dim)\n",
        "actual_values = np.concatenate(actual_values, axis=0)  # Shape: (num_samples, seq_len, output_dim)\n",
        "\n",
        "# Calculate ADE (Average Displacement Error)\n",
        "displacement_errors = np.linalg.norm(predicted_values - actual_values, axis=-1)  # Euclidean distance per sample\n",
        "ade = np.mean(displacement_errors)\n",
        "print(f\"Average Displacement Error (ADE): {ade:.4f}\")\n",
        "\n",
        "# Calculate RMSE (Root Mean Squared Error)\n",
        "rmse = np.sqrt(np.mean((predicted_values - actual_values) ** 2))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "\n",
        "# Display the first 5 predictions vs actuals for quick inspection\n",
        "print(\"\\nPredicted vs Actual (First 5 examples):\")\n",
        "for i in range(5):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(\"Predicted:\", predicted_values[i])\n",
        "    print(\"Actual:\", actual_values[i])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "vMJ_4qM__d3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d450cff-e14f-4ec6-c779-d1e60a37e3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss (MSE): 0.0861\n",
            "Average Displacement Error (ADE): 0.3877\n",
            "Root Mean Squared Error (RMSE): 0.2935\n",
            "\n",
            "Predicted vs Actual (First 5 examples):\n",
            "Example 1:\n",
            "Predicted: [[0.6103173  0.5861249 ]\n",
            " [0.5860547  0.5877527 ]\n",
            " [0.42838    0.52535576]\n",
            " [0.48155162 0.5687767 ]\n",
            " [0.6058507  0.4781041 ]\n",
            " [0.55955875 0.5206224 ]\n",
            " [0.5732668  0.5367383 ]\n",
            " [0.56503147 0.44631457]\n",
            " [0.5265831  0.5013971 ]\n",
            " [0.51184106 0.5807974 ]\n",
            " [0.53728783 0.4941836 ]\n",
            " [0.5414217  0.5516885 ]\n",
            " [0.47824514 0.44760448]\n",
            " [0.4986237  0.485964  ]\n",
            " [0.5109216  0.47854894]\n",
            " [0.57628125 0.4399436 ]\n",
            " [0.49337983 0.47344893]\n",
            " [0.36620638 0.5619882 ]\n",
            " [0.49800384 0.5498748 ]\n",
            " [0.5774192  0.5087586 ]\n",
            " [0.48363572 0.49410135]\n",
            " [0.53478575 0.45704508]\n",
            " [0.5504069  0.46715945]\n",
            " [0.5373987  0.6018098 ]\n",
            " [0.49331626 0.53757626]\n",
            " [0.53031766 0.5727263 ]\n",
            " [0.5037086  0.50271326]\n",
            " [0.5580159  0.5405681 ]\n",
            " [0.4995246  0.45409113]\n",
            " [0.49685147 0.53917545]\n",
            " [0.4905109  0.51345825]\n",
            " [0.4891277  0.5522953 ]]\n",
            "Actual: [[0.7502284  0.32876128]\n",
            " [0.11806089 0.813612  ]\n",
            " [0.85765535 0.06812841]\n",
            " [0.09856313 0.91016334]\n",
            " [0.5188774  0.70835054]\n",
            " [0.30016482 0.74445605]\n",
            " [0.61284816 0.11224729]\n",
            " [0.63925105 0.9158226 ]\n",
            " [0.5101835  0.34623557]\n",
            " [0.03625739 0.01152468]\n",
            " [0.4147219  0.55069846]\n",
            " [0.32082838 0.8902499 ]\n",
            " [0.84921956 0.87347585]\n",
            " [0.17160153 0.09181505]\n",
            " [0.05843252 0.6955014 ]\n",
            " [0.0888024  0.47576088]\n",
            " [0.01501101 0.6109048 ]\n",
            " [0.16275007 0.6625546 ]\n",
            " [0.39337832 0.7181243 ]\n",
            " [0.3430683  0.05443829]\n",
            " [0.39734912 0.6153249 ]\n",
            " [0.43096793 0.43719506]\n",
            " [0.575818   0.9830243 ]\n",
            " [0.89379245 0.7481841 ]\n",
            " [0.8965502  0.9024012 ]\n",
            " [0.3998003  0.9729736 ]\n",
            " [0.6775941  0.5791089 ]\n",
            " [0.11839312 0.8869092 ]\n",
            " [0.9295077  0.7470815 ]\n",
            " [0.25514483 0.41514236]\n",
            " [0.08381575 0.5699493 ]\n",
            " [0.7565573  0.65211993]]\n",
            "\n",
            "Example 2:\n",
            "Predicted: [[0.6111252  0.586822  ]\n",
            " [0.5868988  0.5883104 ]\n",
            " [0.43052018 0.5252787 ]\n",
            " [0.48257583 0.568859  ]\n",
            " [0.606365   0.47887456]\n",
            " [0.56034833 0.5204808 ]\n",
            " [0.5739005  0.53697807]\n",
            " [0.5664151  0.44612002]\n",
            " [0.5269529  0.50214964]\n",
            " [0.51251346 0.5811574 ]\n",
            " [0.5379095  0.49427015]\n",
            " [0.54230404 0.55156094]\n",
            " [0.47978738 0.44789338]\n",
            " [0.49967083 0.48625922]\n",
            " [0.511569   0.4786772 ]\n",
            " [0.5773178  0.4401496 ]\n",
            " [0.49460799 0.473473  ]\n",
            " [0.3684262  0.5643548 ]\n",
            " [0.49899626 0.5500995 ]\n",
            " [0.57849723 0.50945765]\n",
            " [0.48426342 0.494223  ]\n",
            " [0.53547823 0.45788366]\n",
            " [0.5504711  0.46721482]\n",
            " [0.5381463  0.6020017 ]\n",
            " [0.49545974 0.53683335]\n",
            " [0.53186226 0.57268447]\n",
            " [0.5043299  0.5032491 ]\n",
            " [0.5586428  0.540986  ]\n",
            " [0.5010693  0.4543444 ]\n",
            " [0.49845135 0.5396225 ]\n",
            " [0.49104732 0.51364917]\n",
            " [0.491803   0.55372024]]\n",
            "Actual: [[5.69187164e-01 9.67622340e-01]\n",
            " [4.42419231e-01 7.27822900e-01]\n",
            " [7.80016601e-01 4.08223331e-01]\n",
            " [2.56958425e-01 7.28098273e-01]\n",
            " [1.22188926e-02 4.64211762e-01]\n",
            " [2.72463083e-01 7.77339995e-01]\n",
            " [5.78132272e-01 4.31191742e-01]\n",
            " [3.29896212e-02 2.12880373e-02]\n",
            " [9.96487141e-02 5.47614634e-01]\n",
            " [1.71401024e-01 1.18872285e-01]\n",
            " [6.21791720e-01 5.22032440e-01]\n",
            " [3.39707553e-01 1.59706056e-01]\n",
            " [6.01097882e-01 8.86129737e-01]\n",
            " [1.35900736e-01 3.20798814e-01]\n",
            " [3.67038608e-01 1.67936027e-01]\n",
            " [8.39886844e-01 4.95169401e-01]\n",
            " [9.62495208e-02 2.22455859e-02]\n",
            " [5.04250586e-01 8.20444643e-01]\n",
            " [7.91724086e-01 9.76229191e-01]\n",
            " [2.33084083e-01 6.51339233e-01]\n",
            " [3.94961894e-01 3.42113972e-02]\n",
            " [5.74636281e-01 9.45563018e-01]\n",
            " [5.05091369e-01 3.56126308e-01]\n",
            " [7.52674341e-02 1.42407656e-01]\n",
            " [9.14921105e-01 4.23259497e-01]\n",
            " [8.85248184e-04 6.06972754e-01]\n",
            " [3.09578359e-01 7.14171827e-01]\n",
            " [7.18383074e-01 1.78169787e-01]\n",
            " [2.42172062e-01 5.62803090e-01]\n",
            " [1.19998157e-01 2.38073468e-01]\n",
            " [5.38866341e-01 8.12230110e-01]\n",
            " [7.59929240e-01 7.04866052e-02]]\n",
            "\n",
            "Example 3:\n",
            "Predicted: [[0.60999435 0.58680004]\n",
            " [0.5865733  0.5882508 ]\n",
            " [0.42965847 0.52561104]\n",
            " [0.4822284  0.5688463 ]\n",
            " [0.6059599  0.47869337]\n",
            " [0.5600792  0.5205647 ]\n",
            " [0.5734345  0.53718185]\n",
            " [0.5659525  0.44624716]\n",
            " [0.5267103  0.5017998 ]\n",
            " [0.51228493 0.58107996]\n",
            " [0.5376269  0.4941908 ]\n",
            " [0.5418504  0.55167943]\n",
            " [0.4789754  0.4477877 ]\n",
            " [0.49950865 0.48629504]\n",
            " [0.51130575 0.47880483]\n",
            " [0.576921   0.44003052]\n",
            " [0.4944608  0.47351432]\n",
            " [0.36788386 0.564148  ]\n",
            " [0.49862534 0.5500117 ]\n",
            " [0.577723   0.50904065]\n",
            " [0.48393682 0.49430543]\n",
            " [0.5351313  0.45789582]\n",
            " [0.5498503  0.46733195]\n",
            " [0.53787947 0.6019471 ]\n",
            " [0.49516428 0.5369412 ]\n",
            " [0.5308559  0.5726197 ]\n",
            " [0.5040962  0.50319505]\n",
            " [0.55835086 0.54089034]\n",
            " [0.49994206 0.4549088 ]\n",
            " [0.49795496 0.53949094]\n",
            " [0.49081737 0.5136108 ]\n",
            " [0.49068278 0.55332553]]\n",
            "Actual: [[0.04957998 0.9793625 ]\n",
            " [0.79066205 0.5579243 ]\n",
            " [0.76236254 0.33094305]\n",
            " [0.21998012 0.11360353]\n",
            " [0.4254253  0.80549574]\n",
            " [0.8907574  0.9874913 ]\n",
            " [0.6278739  0.17454857]\n",
            " [0.6492789  0.57045156]\n",
            " [0.22619921 0.4084217 ]\n",
            " [0.60367507 0.48052126]\n",
            " [0.38446426 0.9138811 ]\n",
            " [0.7953121  0.00338286]\n",
            " [0.63620615 0.41042286]\n",
            " [0.16189212 0.6050738 ]\n",
            " [0.7184653  0.3351224 ]\n",
            " [0.52270436 0.43246537]\n",
            " [0.8938326  0.36312157]\n",
            " [0.9203647  0.01726955]\n",
            " [0.3600018  0.27962035]\n",
            " [0.4093268  0.2751059 ]\n",
            " [0.510495   0.8722725 ]\n",
            " [0.65041816 0.75449085]\n",
            " [0.6596475  0.00730503]\n",
            " [0.93864167 0.5344569 ]\n",
            " [0.7144094  0.17516792]\n",
            " [0.70895404 0.24817586]\n",
            " [0.1567542  0.16002369]\n",
            " [0.17605525 0.36688286]\n",
            " [0.07591826 0.2824381 ]\n",
            " [0.9202565  0.10323584]\n",
            " [0.22988719 0.424483  ]\n",
            " [0.9584933  0.29606605]]\n",
            "\n",
            "Example 4:\n",
            "Predicted: [[0.61032    0.5865023 ]\n",
            " [0.5867475  0.588176  ]\n",
            " [0.42952335 0.5251308 ]\n",
            " [0.48236084 0.56865066]\n",
            " [0.6061557  0.4784295 ]\n",
            " [0.55997777 0.5204871 ]\n",
            " [0.5736744  0.53684235]\n",
            " [0.56575555 0.4459405 ]\n",
            " [0.526941   0.50172514]\n",
            " [0.5121966  0.58096987]\n",
            " [0.5376993  0.4941666 ]\n",
            " [0.5418804  0.55154955]\n",
            " [0.47922325 0.4476565 ]\n",
            " [0.49964064 0.48619622]\n",
            " [0.5114439  0.47871882]\n",
            " [0.57716876 0.44004184]\n",
            " [0.49455658 0.47348177]\n",
            " [0.36718878 0.5627125 ]\n",
            " [0.4986051  0.5500885 ]\n",
            " [0.5778769  0.50949883]\n",
            " [0.48397228 0.49409103]\n",
            " [0.5354435  0.45742607]\n",
            " [0.5508508  0.46714222]\n",
            " [0.53788924 0.6019332 ]\n",
            " [0.49536607 0.5367984 ]\n",
            " [0.5312635  0.5726523 ]\n",
            " [0.5041104  0.5030452 ]\n",
            " [0.5582556  0.5420232 ]\n",
            " [0.5006297  0.45429593]\n",
            " [0.49809992 0.53950936]\n",
            " [0.49087274 0.51358473]\n",
            " [0.49063691 0.55312   ]]\n",
            "Actual: [[0.45009273 0.08818531]\n",
            " [0.44069004 0.34362775]\n",
            " [0.17333084 0.25674045]\n",
            " [0.20320195 0.1390112 ]\n",
            " [0.52698725 0.6188207 ]\n",
            " [0.9868262  0.48488152]\n",
            " [0.9775394  0.86886287]\n",
            " [0.47516304 0.43987066]\n",
            " [0.70112073 0.33876783]\n",
            " [0.257591   0.13867813]\n",
            " [0.82322955 0.2606163 ]\n",
            " [0.08248979 0.26770717]\n",
            " [0.24350333 0.5351768 ]\n",
            " [0.7667118  0.33249944]\n",
            " [0.73169774 0.339324  ]\n",
            " [0.19388515 0.29525584]\n",
            " [0.05150098 0.7276724 ]\n",
            " [0.35057575 0.44378155]\n",
            " [0.243657   0.5900809 ]\n",
            " [0.5002263  0.44197434]\n",
            " [0.91303146 0.7327255 ]\n",
            " [0.76241904 0.90218633]\n",
            " [0.38489097 0.03328097]\n",
            " [0.48136717 0.24879014]\n",
            " [0.4829654  0.40361434]\n",
            " [0.41760528 0.8831559 ]\n",
            " [0.26521796 0.3279857 ]\n",
            " [0.26733512 0.33003682]\n",
            " [0.68263805 0.89506316]\n",
            " [0.33558178 0.5307284 ]\n",
            " [0.16191    0.40263337]\n",
            " [0.9225244  0.19571024]]\n",
            "\n",
            "Example 5:\n",
            "Predicted: [[0.61010975 0.5866916 ]\n",
            " [0.58658934 0.5883737 ]\n",
            " [0.42980915 0.5259524 ]\n",
            " [0.48231918 0.5689489 ]\n",
            " [0.606075   0.478804  ]\n",
            " [0.5601123  0.5205535 ]\n",
            " [0.57353294 0.5371685 ]\n",
            " [0.5662582  0.44645017]\n",
            " [0.5264847  0.5018397 ]\n",
            " [0.5122937  0.58109015]\n",
            " [0.5375639  0.4941935 ]\n",
            " [0.54200363 0.5516449 ]\n",
            " [0.47873583 0.44781327]\n",
            " [0.49958336 0.48637712]\n",
            " [0.5113119  0.4788044 ]\n",
            " [0.5768604  0.43999255]\n",
            " [0.4941231  0.47348386]\n",
            " [0.3677907  0.5642763 ]\n",
            " [0.49870726 0.5500922 ]\n",
            " [0.57780373 0.5092933 ]\n",
            " [0.4839528  0.49425226]\n",
            " [0.5351953  0.45816743]\n",
            " [0.5498484  0.46734875]\n",
            " [0.53779984 0.60193694]\n",
            " [0.49501118 0.5368883 ]\n",
            " [0.53071785 0.5723548 ]\n",
            " [0.5040108  0.5031294 ]\n",
            " [0.5583406  0.54095984]\n",
            " [0.5003899  0.45498282]\n",
            " [0.49797222 0.5394407 ]\n",
            " [0.49088043 0.5136983 ]\n",
            " [0.49146056 0.55379075]]\n",
            "Actual: [[0.9783545  0.28369117]\n",
            " [0.33011556 0.24983358]\n",
            " [0.92873836 0.8222835 ]\n",
            " [0.3761713  0.36641127]\n",
            " [0.8131414  0.08385873]\n",
            " [0.43089128 0.9492129 ]\n",
            " [0.8925502  0.3795302 ]\n",
            " [0.17037725 0.00973225]\n",
            " [0.47056317 0.9432626 ]\n",
            " [0.35265946 0.0215385 ]\n",
            " [0.5124705  0.6217396 ]\n",
            " [0.98545235 0.9241318 ]\n",
            " [0.22074026 0.91345435]\n",
            " [0.62407684 0.38223094]\n",
            " [0.39035004 0.60717267]\n",
            " [0.5009744  0.06526917]\n",
            " [0.50902945 0.7390984 ]\n",
            " [0.5284587  0.30211043]\n",
            " [0.5673412  0.41277707]\n",
            " [0.9709969  0.749614  ]\n",
            " [0.75274587 0.36200333]\n",
            " [0.34019047 0.8031284 ]\n",
            " [0.4722857  0.67749166]\n",
            " [0.7762765  0.00480109]\n",
            " [0.2926709  0.50423604]\n",
            " [0.31516153 0.31593055]\n",
            " [0.8476548  0.67905134]\n",
            " [0.5086666  0.59882754]\n",
            " [0.15635717 0.38943285]\n",
            " [0.3964839  0.9628205 ]\n",
            " [0.11194414 0.26441133]\n",
            " [0.20084512 0.88401866]]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}