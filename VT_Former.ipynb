{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnmQ2tpX2eDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68d14ea9-d35e-42ec-ad20-e2aa19d340eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt20cu118\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_sparse-0.6.18%2Bpt20cu118-cp310-cp310-linux_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt20cu118\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.10.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.66.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.8.30)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->torch-geometric) (0.2.0)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RomJ1dSwYNP7",
        "outputId": "9ae97b7d-3a40-438d-9faa-f191245c211a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir('/content/drive/MyDrive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrdR512FAo4z",
        "outputId": "dfbda2bc-c224-4039-f3eb-a6bb01974a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Resumé Sample ',\n",
              " 'Books',\n",
              " 'IMG_20201113_095042_1.jpg',\n",
              " 'Classroom',\n",
              " 'GATE',\n",
              " 'IMG_20210413_105548 (1).jpg',\n",
              " 'IMG_20210413_105548.jpg',\n",
              " 'Sheet 6.pdf',\n",
              " 'Vaccine Certificate(2).pdf',\n",
              " 'Quant Trading',\n",
              " 'Internship Resume.docx',\n",
              " 'Colab Notebooks',\n",
              " 'CAT',\n",
              " 'Volume-2.pdf',\n",
              " 'Volume-3.pdf',\n",
              " 'hrd_ppt_group9.pdf',\n",
              " 'Flow-Chart.drawio',\n",
              " 'Flow-Chart.drawio.xml',\n",
              " 'Flow-Chart.png',\n",
              " 'Certificates',\n",
              " 'Copy of amazon.gsheet',\n",
              " 'MY_RESUME',\n",
              " 'CV_Rohit_Chakraborty.pdf',\n",
              " '11500120030_Rohit_Chakraborty.pdf',\n",
              " 'Cron_Job.drawio.png',\n",
              " '11500120030_Rohit_Chakraborty_CA1.pdf',\n",
              " 'Contacts',\n",
              " 'you have missed oral solution column recreate the....gsheet',\n",
              " 'KiPhyNet_ACS_synthetic_biology-2.pdf',\n",
              " 'admit_card.pdf',\n",
              " 'ALL PDF RECEIVED FROM INSTITUTE IN ADMISSION.pdf',\n",
              " 'APPLICANTS GRADUATION DOCS.pdf',\n",
              " 'APPLICANTS KYC PAN CARD.pdf',\n",
              " 'APPLICANTS KYC AADHAR.pdf',\n",
              " 'APPLICANTS X DOCS.pdf',\n",
              " 'APPLICANTS XII DOCS.pdf',\n",
              " 'LOAN APPLICATION FORM VIDYALAXMI.pdf',\n",
              " 'FEES STRUCTURE.pdf',\n",
              " 'INSTITUTE BROCHURE.pdf',\n",
              " 'Research Paper(IISc).pdf',\n",
              " 'NIT SURATHKAL ',\n",
              " 'Cormen, Thomas H. - Introduction to Algorithms (3rd ed.) - libgen.li.pdf',\n",
              " 'Dataset',\n",
              " 'PROVISIONAL CERTIFICATE(BTech).pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "path=\"/content/drive/MyDrive/Dataset/Dataset.csv\"\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(path, nrows = 3000)\n",
        "    print(\"Data loaded successfully!\")\n",
        "\n",
        "    # Step 5: Display the first 5 rows of the DataFrame\n",
        "    print(df.head())\n",
        "\n",
        "    print(f\"Number of rows: {df.shape[0]}, Number of columns: {df.shape[1]}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the path and try again.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PaF41mYY_VE",
        "outputId": "d88a376f-9612-4cf8-afa5-5b7d84c3caa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n",
            "   Vehicle ID  Frame ID  Total Frames   Global Time  Local X  Local Y  \\\n",
            "0           2        14           437  1.118850e+12   16.447   39.381   \n",
            "1           2        15           437  1.118850e+12   16.426   43.381   \n",
            "2           2        16           437  1.118850e+12   16.405   47.380   \n",
            "3           2        17           437  1.118850e+12   16.385   51.381   \n",
            "4           2        18           437  1.118850e+12   16.364   55.381   \n",
            "\n",
            "      Global X     Global Y  Vehicle Length  Vehicle Width  Vehicle Class  \\\n",
            "0  6451140.329  1873342.000            14.5            4.9              2   \n",
            "1  6451143.018  1873339.038            14.5            4.9              2   \n",
            "2  6451145.706  1873336.077            14.5            4.9              2   \n",
            "3  6451148.395  1873333.115            14.5            4.9              2   \n",
            "4  6451151.084  1873330.153            14.5            4.9              2   \n",
            "\n",
            "   Vehicle Velocity  Vehicle Accl  Lane Identification  Preceeding Vehicle  \\\n",
            "0              40.0           0.0                    2                   0   \n",
            "1              40.0           0.0                    2                   0   \n",
            "2              40.0           0.0                    2                   0   \n",
            "3              40.0           0.0                    2                   0   \n",
            "4              40.0           0.0                    2                   0   \n",
            "\n",
            "   Following Vehicle  Spacing  Headway  \n",
            "0                  0      0.0      0.0  \n",
            "1                  0      0.0      0.0  \n",
            "2                  0      0.0      0.0  \n",
            "3                  0      0.0      0.0  \n",
            "4                  0      0.0      0.0  \n",
            "Number of rows: 3000, Number of columns: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare the nodes\n",
        "node_features = df[['Local X', 'Local Y', 'Vehicle Velocity', 'Vehicle Accl']]\n",
        "node_features = (node_features - node_features.mean()) / node_features.std()\n",
        "node_features_tensor = torch.tensor(node_features.values, dtype=torch.float)"
      ],
      "metadata": {
        "id": "f-ZxPf0ELCHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFirst 5 Node Features:\")\n",
        "print(node_features_tensor[:5])  # Change 5 to any number to see more/less nodes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJO0oGzTzMwl",
        "outputId": "c3141506-fc6a-4cf0-ef9c-42f4980c0d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 5 Node Features:\n",
            "tensor([[-0.8727, -1.6897, -0.9491, -0.0810],\n",
            "        [-0.8738, -1.6830, -0.9491, -0.0810],\n",
            "        [-0.8749, -1.6762, -0.9491, -0.0810],\n",
            "        [-0.8760, -1.6694, -0.9491, -0.0810],\n",
            "        [-0.8771, -1.6626, -0.9491, -0.0810]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create edges based on Preceding/Following Vehicle relationships and a proximity radius\n",
        "def create_edges_with_proximity(df, radius):\n",
        "    edge_index = []\n",
        "\n",
        "    # Iterate through the dataframe to establish edges between vehicles\n",
        "    for i, row in df.iterrows():\n",
        "        preceding_vehicle = row['Preceeding Vehicle']\n",
        "        following_vehicle = row['Following Vehicle']\n",
        "\n",
        "        # Helper function to check distance and create edge\n",
        "        def add_edge_if_within_radius(i, vehicle_id):\n",
        "            if vehicle_id != 0:\n",
        "                vehicle_index = df[df['Vehicle ID'] == vehicle_id].index\n",
        "                if len(vehicle_index) > 0:\n",
        "                    vehicle_index = vehicle_index[0]\n",
        "\n",
        "                    # Calculate the Euclidean distance between the vehicles\n",
        "                    distance = np.sqrt((row['Local X'] - df.loc[vehicle_index]['Local X'])**2 +\n",
        "                                       (row['Local Y'] - df.loc[vehicle_index]['Local Y'])**2)\n",
        "\n",
        "                    # If within the radius, create bidirectional edges\n",
        "                    if distance <= radius:\n",
        "                        edge_index.append([i, vehicle_index])\n",
        "                        edge_index.append([vehicle_index, i])  # Add both directions for an undirected graph\n",
        "\n",
        "        # Check and add edges for the preceding vehicle\n",
        "        add_edge_if_within_radius(i, preceding_vehicle)\n",
        "\n",
        "        # Check and add edges for the following vehicle\n",
        "        add_edge_if_within_radius(i, following_vehicle)\n",
        "\n",
        "    # Convert edge_index to tensor and transpose it to (2, num_edges)\n",
        "    return torch.tensor(edge_index, dtype=torch.long).t()\n",
        "\n",
        "# Define a proximity radius (e.g., 100 feet)\n",
        "proximity_radius = 100\n",
        "\n",
        "# Step 4: Create edge index (edges) based on Preceding/Following Vehicle and proximity\n",
        "edge_index = create_edges_with_proximity(df, proximity_radius)\n",
        "\n",
        "# Step 5: Create a PyTorch Geometric Data object\n",
        "data = Data(x=node_features_tensor, edge_index=edge_index)\n",
        "\n",
        "# Step 6: Check the Data object\n",
        "print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iXgIb1Vzb4G",
        "outputId": "3ccbafc4-6c96-4db9-8067-1a46e6bb1fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[3000, 4], edge_index=[2, 180])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the GATNet Module\n",
        "class GATNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_heads):\n",
        "        super(GATNet, self).__init__()\n",
        "        self.gat1 = GATConv(input_dim, hidden_dim, heads=num_heads, concat=True, dropout=0.2)\n",
        "        self.gat2 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, concat=True, dropout=0.2)\n",
        "        self.gat3 = GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads, concat=True, dropout=0.2)\n",
        "        self.gat4 = GATConv(hidden_dim * num_heads, hidden_dim, heads=1, concat=False)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)  # Match output_dim with Transformer input_dim\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.elu(self.gat1(x, edge_index))\n",
        "        x = F.elu(self.gat2(x, edge_index))\n",
        "        x = F.elu(self.gat3(x, edge_index))\n",
        "        x = F.elu(self.gat4(x, edge_index))\n",
        "        x = self.fc(x)  # Final output dimension should match Transformer’s input_dim\n",
        "        return x"
      ],
      "metadata": {
        "id": "Xq4Zhho09xNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Model Hyperparameters\n",
        "input_dim = 4  # Number of node features (Local X, Local Y, Vehicle Velocity, Vehicle Accl)\n",
        "hidden_dim = 32  # Number of hidden units in GAT layers\n",
        "output_dim = 2  # Predicting 2D coordinates (x, y)\n",
        "num_heads = 4   # Number of attention heads"
      ],
      "metadata": {
        "id": "BmO-IVZD5uWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = GATNet(input_dim, hidden_dim, output_dim, num_heads)"
      ],
      "metadata": {
        "id": "gntLJ3BP55p9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Optimizer and Loss Function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "loss_fn = torch.nn.MSELoss()  # Using Mean Squared Error for regression (predicting coordinates)"
      ],
      "metadata": {
        "id": "ZVmyapz258_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Add the next positions (Next X, Next Y) as the target labels\n",
        "df['Next X'] = df.groupby('Vehicle ID')['Local X'].shift(-1)\n",
        "df['Next Y'] = df.groupby('Vehicle ID')['Local Y'].shift(-1)\n",
        "\n",
        "# Remove NaN values (last row of each vehicle will not have a next position)\n",
        "df = df.dropna(subset=['Next X', 'Next Y'])\n",
        "\n",
        "# Standardize node features (if not done previously)\n",
        "scaler_features = StandardScaler()\n",
        "node_features = scaler_features.fit_transform(df[['Local X', 'Local Y', 'Vehicle Velocity', 'Vehicle Accl']].values)\n",
        "node_features_tensor = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "# Standardize target values for Next X and Next Y\n",
        "scaler_target = StandardScaler()\n",
        "target = scaler_target.fit_transform(df[['Next X', 'Next Y']].values)\n",
        "target_tensor = torch.tensor(target, dtype=torch.float)\n",
        "\n",
        "# Create the PyTorch Geometric Data object with the standardized targets\n",
        "data = Data(x=node_features_tensor, edge_index=edge_index, y=target_tensor)"
      ],
      "metadata": {
        "id": "_mbPpniQEapQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the node features and targets into training and testing sets\n",
        "train_mask, test_mask = train_test_split(np.arange(len(df)), test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Reindex edge_index for training data ---\n",
        "# 1. Filter edges based on train_mask (instead of indexing directly)\n",
        "train_edge_index = edge_index[:, np.isin(edge_index[0], train_mask) & np.isin(edge_index[1], train_mask)]\n",
        "\n",
        "# 2. Get unique node indices in the filtered training set\n",
        "train_nodes = np.unique(train_edge_index)\n",
        "\n",
        "# 3. Create a mapping from original node indices to new indices in the training set\n",
        "node_map = {node: i for i, node in enumerate(train_nodes)}\n",
        "\n",
        "# 4. Reindex train_edge_index using the mapping\n",
        "train_edge_index = torch.tensor([[node_map[n1], node_map[n2]]\n",
        "                                  for n1, n2 in train_edge_index.t().tolist()], dtype=torch.long).t()\n",
        "\n",
        "# --- Repeat the same process for test data ---\n",
        "# 1. Filter edges based on test_mask\n",
        "test_edge_index = edge_index[:, np.isin(edge_index[0], test_mask) & np.isin(edge_index[1], test_mask)]\n",
        "\n",
        "# 2. Get unique node indices in the filtered test set\n",
        "test_nodes = np.unique(test_edge_index)\n",
        "\n",
        "# 3. Create a mapping from original node indices to new indices in the test set\n",
        "node_map = {node: i for i, node in enumerate(test_nodes)}\n",
        "\n",
        "# 4. Reindex test_edge_index using the mapping\n",
        "test_edge_index = torch.tensor([[node_map[n1], node_map[n2]]\n",
        "                                 for n1, n2 in test_edge_index.t().tolist()], dtype=torch.long).t()\n",
        "\n",
        "\n",
        "train_data = Data(x=node_features_tensor[train_mask],\n",
        "                  edge_index=train_edge_index,  # Use reindexed edge_index\n",
        "                  y=target_tensor[train_mask])\n",
        "\n",
        "test_data = Data(x=node_features_tensor[test_mask],\n",
        "                 edge_index=test_edge_index,   # Use reindexed edge_index\n",
        "                 y=target_tensor[test_mask])"
      ],
      "metadata": {
        "id": "eriuxYLUE2i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Define a training loop\n",
        "def train_model(model, data, epochs=100):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "\n",
        "        # Forward pass through the model\n",
        "        out = model(data)\n",
        "\n",
        "        # Assuming the ground truth for trajectory is in the Data object (use actual labels in practice)\n",
        "        target = data.y  # Placeholder for ground truth labels\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = loss_fn(out, target)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print loss at every epoch\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n"
      ],
      "metadata": {
        "id": "RMFcqhED6Bml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_data):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient computation for faster testing\n",
        "        out = model(test_data)  # Make predictions on the test data\n",
        "\n",
        "        # Compute the test loss (MSE)\n",
        "        test_loss = loss_fn(out, test_data.y)\n",
        "        print(f'Test Loss (MSE): {test_loss.item()}')\n",
        "\n",
        "        # Convert predictions and actual values to numpy arrays\n",
        "        predicted = out.numpy()\n",
        "        actual = test_data.y.numpy()\n",
        "\n",
        "        # Calculate ADE (Average Displacement Error)\n",
        "        # ADE is the mean Euclidean distance between each predicted and actual trajectory point\n",
        "        displacement_errors = np.linalg.norm(predicted - actual, axis=1)\n",
        "        ade = np.mean(displacement_errors)\n",
        "        print(f'Average Displacement Error (ADE): {ade}')\n",
        "\n",
        "        # Calculate RMSE (Root Mean Squared Error)\n",
        "        rmse = np.sqrt(np.mean((predicted - actual) ** 2))\n",
        "        print(f'Root Mean Squared Error (RMSE): {rmse}')\n",
        "\n",
        "        # Display predicted vs actual for the first 5 examples\n",
        "        print(\"\\nPredicted vs Actual (First 5 examples):\")\n",
        "        print(\"Predicted:\", predicted[:5])\n",
        "        print(\"Actual:\", actual[:5])"
      ],
      "metadata": {
        "id": "ZfstDte26J8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, train the model\n",
        "train_model(model, train_data, epochs=100)\n",
        "\n",
        "# After training, evaluate the model on the test set\n",
        "test_model(model, test_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6nKgFYmFljK",
        "outputId": "983954dd-bee4-4c9a-ec97-08c34ed81854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.9661008715629578\n",
            "Epoch 10, Loss: 0.7519545555114746\n",
            "Epoch 20, Loss: 0.5786685347557068\n",
            "Epoch 30, Loss: 0.4400305151939392\n",
            "Epoch 40, Loss: 0.3415727913379669\n",
            "Epoch 50, Loss: 0.26475584506988525\n",
            "Epoch 60, Loss: 0.2304309755563736\n",
            "Epoch 70, Loss: 0.20678040385246277\n",
            "Epoch 80, Loss: 0.18840709328651428\n",
            "Epoch 90, Loss: 0.18562999367713928\n",
            "Test Loss (MSE): 0.0424208864569664\n",
            "Average Displacement Error (ADE): 0.22184643149375916\n",
            "Root Mean Squared Error (RMSE): 0.20596332848072052\n",
            "\n",
            "Predicted vs Actual (First 5 examples):\n",
            "Predicted: [[-1.0318152   0.1271076 ]\n",
            " [-0.9997838  -0.24657339]\n",
            " [-0.8313875  -0.27028987]\n",
            " [ 0.41209882 -0.23149526]\n",
            " [ 0.49535692 -0.21626493]]\n",
            "Actual: [[-1.2866697   1.5843787 ]\n",
            " [-1.4149683  -0.00394412]\n",
            " [ 1.2027675   0.01464173]\n",
            " [-0.90751415 -1.3513286 ]\n",
            " [ 0.22017324 -1.5468384 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Transformer Module*"
      ],
      "metadata": {
        "id": "xSWH4RZF5mXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "\n",
        "# Define the DecoderOnlyTransformer Module\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, seq_len, input_dim, embed_dim, num_heads, ff_dim, num_layers, output_dim):\n",
        "        super(DecoderOnlyTransformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embedding = nn.Linear(input_dim, embed_dim)  # Embedding layer to match GATNet output to embed_dim\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_len, embed_dim))  # Positional encoding\n",
        "\n",
        "        # Transformer decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Linear layer to map the final output to desired output_dim (2 for (x, y))\n",
        "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Embed GATNet output and add positional encoding\n",
        "        embed_dim=32\n",
        "        seq_len=10\n",
        "        device = x.device\n",
        "        #self.embedding = nn.Linear(input_dim, embed_dim)\n",
        "        # Change here: seq_len should be num_nodes\n",
        "        #self.positional_encoding = nn.Parameter(torch.randn(1, seq_len, embed_dim))\n",
        "        self.embedding = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        batch_size = x.shape[0]  # Get batch size from input\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, self.embed_dim, self.embed_dim), requires_grad=True).to(device)\n",
        "        positional_encoding = self.positional_encoding.repeat(batch_size, 1, 1)  # Repeat for batch size\n",
        "        #self.positional_encoding = nn.Parameter(torch.randn(1, x.shape[1], self.embed_dim)).to(x.device)\n",
        "\n",
        "        #x = self.embedding(x) + self.positional_encoding\n",
        "        x = self.embedding(x) + positional_encoding[:, :x.shape[1], :]\n",
        "\n",
        "        # Pass through each decoder layer\n",
        "        for layer in self.decoder_layers:\n",
        "            x = layer(x, x)  # Both tgt and memory are x for decoder-only\n",
        "\n",
        "        # Project to the output dimension (e.g., 2 for (x, y) positions)\n",
        "        output = self.fc_out(x)\n",
        "        return output\n",
        "\n",
        "# Sample Execution Code\n",
        "\n",
        "# Instantiate the GAT and Transformer models\n",
        "gat_model = GATNet(input_dim=4, hidden_dim=32, output_dim=32, num_heads=4)  # output_dim = 32 to match Transformer’s embed_dim\n",
        "transformer_model = DecoderOnlyTransformer(seq_len=10, input_dim=32, embed_dim=32, num_heads=4, ff_dim=128, num_layers=8, output_dim=2)\n",
        "\n",
        "# Sample data for GATNet\n",
        "from torch_geometric.data import Data\n",
        "input_dim = 4\n",
        "num_nodes = 32\n",
        "x = torch.rand((num_nodes, input_dim))  # Random node features\n",
        "edge_index = torch.randint(0, num_nodes, (2, 150))  # Random edges for the graph\n",
        "data = Data(x=x, edge_index=edge_index)\n",
        "\n",
        "# Run GATNet to obtain embeddings\n",
        "gat_output = gat_model(data)  # Shape: (num_nodes, 32) - (32 is the embed_dim for Transformer)\n",
        "\n",
        "# Reshape GATNet output to match Transformer input\n",
        "gat_output = gat_output.unsqueeze(0)  # Add batch dimension, shape: (1, num_nodes, 32)\n",
        "\n",
        "# Run Transformer on GATNet output\n",
        "output = transformer_model(gat_output)\n",
        "print(\"Output shape:\", output.shape)  # Expected shape: (1, num_nodes, 2)\n"
      ],
      "metadata": {
        "id": "qVqAspxI95R2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6b9a60d-6dd8-49c2-bdb8-23cb704fa75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 32, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Load and Preprocess the Dataset*"
      ],
      "metadata": {
        "id": "iymcnMNY4-Iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads):\n",
        "        super(DecoderOnlyTransformer, self).__init__()\n",
        "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
        "        self.transformer = nn.TransformerDecoder(\n",
        "            nn.TransformerDecoderLayer(hidden_dim, num_heads),\n",
        "            num_layers\n",
        "        )\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "        self.positional_encoding = self.generate_positional_encoding(hidden_dim, 5000)  # Increased max_len\n",
        "\n",
        "\n",
        "    def generate_positional_encoding(self, d_model, max_len):\n",
        "        \"\"\"\n",
        "        Generate positional encoding for Transformer.\n",
        "\n",
        "        Args:\n",
        "            d_model: Dimension of the model.\n",
        "            max_len: Maximum sequence length.\n",
        "\n",
        "        Returns:\n",
        "            Positional encoding tensor of shape (max_len, d_model).\n",
        "        \"\"\"\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Add batch dimension\n",
        "        return pe\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the DecoderOnlyTransformer.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            Output tensor of shape (batch_size, sequence_length, output_dim).\n",
        "        \"\"\"\n",
        "        # Reshape before linear layer\n",
        "        x = x.reshape(-1, self.embedding.in_features)\n",
        "\n",
        "        x = self.embedding(x) + self.positional_encoding[:, :x.shape[0], :]\n",
        "\n",
        "        # Reshape for TransformerDecoder\n",
        "        batch_size = x.shape[0] // num_nodes  # Assuming num_nodes is a global variable\n",
        "        x = x.view(num_nodes, batch_size, -1)  # (sequence_length, batch_size, embedding_dim)\n",
        "\n",
        "        # TransformerDecoder expects (sequence_length, batch_size, embedding_dim)\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Reshape back to (batch_size * num_nodes, output_dim)\n",
        "        x = x.view(batch_size * num_nodes, -1)\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "'''"
      ],
      "metadata": {
        "id": "hNggnWXLChy1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "76373af2-d22d-40bb-dc70-4f146d86a04a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nimport torch.nn as nn\\n\\nclass DecoderOnlyTransformer(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_heads):\\n        super(DecoderOnlyTransformer, self).__init__()\\n        self.embedding = nn.Linear(input_dim, hidden_dim)\\n        self.transformer = nn.TransformerDecoder(\\n            nn.TransformerDecoderLayer(hidden_dim, num_heads),\\n            num_layers\\n        )\\n        self.fc_out = nn.Linear(hidden_dim, output_dim)\\n        self.positional_encoding = self.generate_positional_encoding(hidden_dim, 5000)  # Increased max_len\\n\\n\\n    def generate_positional_encoding(self, d_model, max_len):\\n        \"\"\"\\n        Generate positional encoding for Transformer.\\n\\n        Args:\\n            d_model: Dimension of the model.\\n            max_len: Maximum sequence length.\\n\\n        Returns:\\n            Positional encoding tensor of shape (max_len, d_model).\\n        \"\"\"\\n        pe = torch.zeros(max_len, d_model)\\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\\n        pe[:, 0::2] = torch.sin(position * div_term)\\n        pe[:, 1::2] = torch.cos(position * div_term)\\n        pe = pe.unsqueeze(0)  # Add batch dimension\\n        return pe\\n\\n    def forward(self, x):\\n        \"\"\"\\n        Forward pass of the DecoderOnlyTransformer.\\n\\n        Args:\\n            x: Input tensor of shape (batch_size, sequence_length, input_dim).\\n\\n        Returns:\\n            Output tensor of shape (batch_size, sequence_length, output_dim).\\n        \"\"\"\\n        # Reshape before linear layer\\n        x = x.reshape(-1, self.embedding.in_features)\\n\\n        x = self.embedding(x) + self.positional_encoding[:, :x.shape[0], :]\\n\\n        # Reshape for TransformerDecoder\\n        batch_size = x.shape[0] // num_nodes  # Assuming num_nodes is a global variable\\n        x = x.view(num_nodes, batch_size, -1)  # (sequence_length, batch_size, embedding_dim)\\n\\n        # TransformerDecoder expects (sequence_length, batch_size, embedding_dim)\\n        x = self.transformer(x)\\n\\n        # Reshape back to (batch_size * num_nodes, output_dim)\\n        x = x.view(batch_size * num_nodes, -1)\\n        x = self.fc_out(x)\\n        return x\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch.nn as nn\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load dataset\n",
        "data_path = path\n",
        "data = pd.read_csv(path)\n",
        "\n",
        "# Take a random sample of 3000 rows\n",
        "sampled_data = data.sample(n=3000, random_state=42)  # random_state for reproducibility\n",
        "\n",
        "sampled_data['Next X'] = sampled_data.groupby('Vehicle ID')['Local X'].shift(-1)\n",
        "sampled_data['Next Y'] = sampled_data.groupby('Vehicle ID')['Local Y'].shift(-1)\n",
        "sampled_data = sampled_data.dropna(subset=['Next X', 'Next Y'])\n",
        "\n",
        "# Preprocess data\n",
        "input_columns = ['Local X', 'Local Y', 'Vehicle Velocity', 'Vehicle Accl']\n",
        "target_columns = ['Next X','Next Y']\n",
        "\n",
        "all_columns = input_columns + target_columns\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "sampled_data[all_columns] = scaler.fit_transform(sampled_data[all_columns])\n",
        "'''\n",
        "sampled_data[input_columns] = scaler.fit_transform(sampled_data[input_columns])\n",
        "sampled_data[target_columns] = scaler.transform(sampled_data[target_columns])\n",
        "'''\n",
        "\n",
        "# Convert to torch tensors\n",
        "X = torch.tensor(sampled_data[input_columns].values, dtype=torch.float32) #input\n",
        "y = torch.tensor(sampled_data[target_columns].values, dtype=torch.float32) #output\n",
        "\n",
        "# Reshape data to fit (samples, sequence length, input/output dim)\n",
        "seq_len = 10\n",
        "num_samples = X.shape[0] // seq_len\n",
        "X = X[:num_samples * seq_len].view(num_samples, seq_len, -1)  # Shape: (samples, seq_len, input_dim)\n",
        "y = y[:num_samples * seq_len].view(num_samples, seq_len, -1)  # Shape: (samples, seq_len, output_dim)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 8\n",
        "train_data = TensorDataset(X, y)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "AO1YxXYM1M6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X contains normalized inputs and y contains normalized targets\n",
        "\n",
        "# Input data statistics\n",
        "print(\"Input Data Statistics:\")\n",
        "print(\"Min:\", X.min().item(), \"Max:\", X.max().item())\n",
        "print(\"Mean:\", X.mean().item(), \"Std Dev:\", X.std().item())\n",
        "\n",
        "# Target data statistics\n",
        "print(\"\\nTarget Data Statistics:\")\n",
        "print(\"Min:\", y.min().item(), \"Max:\", y.max().item())\n",
        "print(\"Mean:\", y.mean().item(), \"Std Dev:\", y.std().item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFNtmDYw-UMm",
        "outputId": "49f0a8f9-2b9f-4f12-89d2-108c1b46d6ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Data Statistics:\n",
            "Min: 0.0 Max: 1.0\n",
            "Mean: 0.46156561374664307 Std Dev: 0.24507516622543335\n",
            "\n",
            "Target Data Statistics:\n",
            "Min: 0.0 Max: 1.0\n",
            "Mean: 0.41317644715309143 Std Dev: 0.27452149987220764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*split the data form training,testing and validation*"
      ],
      "metadata": {
        "id": "vhk8dGeH6ukb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Define the split sizes (e.g., 60% train, 20% validation, 20% test)\n",
        "train_size = int(0.6 * len(train_data))\n",
        "val_size = int(0.2 * len(train_data))\n",
        "test_size = len(train_data) - train_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(train_data, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "'''"
      ],
      "metadata": {
        "id": "-HxxmqwS6tcD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "503882e9-cf74-4c0b-c95a-61750252dcc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Define the split sizes (e.g., 60% train, 20% validation, 20% test)\\ntrain_size = int(0.6 * len(train_data))\\nval_size = int(0.2 * len(train_data))\\ntest_size = len(train_data) - train_size - val_size\\n\\n# Split the dataset\\ntrain_dataset, val_dataset, test_dataset = random_split(train_data, [train_size, val_size, test_size])\\n\\n# Create DataLoaders for each set\\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Training Loop*"
      ],
      "metadata": {
        "id": "x9E-mXdS5AlZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data_list in train_loader:\n",
        "    print([type(item) for item in data_list])  # Check types in data_list\n",
        "    break  # Check the first batch only"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clMNqHjl6IjJ",
        "outputId": "b51de660-aeb3-48e7-f3b1-b71579f43771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<class 'torch.Tensor'>, <class 'torch.Tensor'>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "num_edges=150\n",
        "# Example tensors for a single graph sample\n",
        "x = torch.rand((num_nodes, input_dim))  # Node features tensor\n",
        "edge_index = torch.randint(0, num_nodes, (2, num_edges))  # Edge indices tensor\n",
        "y = torch.rand((num_nodes, output_dim))  # Target values tensor\n",
        "\n",
        "# Create a PyTorch Geometric Data object\n",
        "data = Data(x=x, edge_index=edge_index, y=y)\n"
      ],
      "metadata": {
        "id": "Rwktefhd7kkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Assume `data_list` contains multiple `Data` objects, one for each graph\n",
        "data_list = [Data(x=torch.rand((num_nodes, input_dim)),\n",
        "                  edge_index=torch.randint(0, num_nodes, (2, num_edges)),\n",
        "                  y=torch.rand((num_nodes, output_dim))) for _ in range(num_samples)]\n",
        "'''"
      ],
      "metadata": {
        "id": "uwRRRjry8L0H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "74bc402e-f8ec-49e4-a922-2493e353c9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Assume `data_list` contains multiple `Data` objects, one for each graph\\ndata_list = [Data(x=torch.rand((num_nodes, input_dim)),\\n                  edge_index=torch.randint(0, num_nodes, (2, num_edges)),\\n                  y=torch.rand((num_nodes, output_dim))) for _ in range(num_samples)]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "# Create a DataLoader with batching enabled\n",
        "\n",
        "train_size = int(0.6 * len(train_data))\n",
        "val_size = int(0.2 * len(train_data))\n",
        "test_size = len(train_data) - train_size - val_size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, val_dataset, test_dataset = random_split(train_data, [train_size, val_size, test_size])\n",
        "\n",
        "# Create DataLoaders for each set\n",
        "train_loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(data_list, batch_size=32, shuffle=False)\n",
        "'''"
      ],
      "metadata": {
        "id": "T2WiZDYn8Q0D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "08afd151-93c4-4171-9ce1-e0b108b29bde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfrom torch_geometric.data import DataLoader\\n\\n# Create a DataLoader with batching enabled\\n\\ntrain_size = int(0.6 * len(train_data))\\nval_size = int(0.2 * len(train_data))\\ntest_size = len(train_data) - train_size - val_size\\n\\n# Split the dataset\\ntrain_dataset, val_dataset, test_dataset = random_split(train_data, [train_size, val_size, test_size])\\n\\n# Create DataLoaders for each set\\ntrain_loader = DataLoader(data_list, batch_size=32, shuffle=True)\\nval_loader = DataLoader(data_list, batch_size=32, shuffle=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data, DataLoader,Batch\n",
        "\n",
        "# Example data creation\n",
        "num_nodes = 32\n",
        "input_dim = 4\n",
        "output_dim = 2\n",
        "num_edges =150\n",
        "\n",
        "# Generate some example graphs\n",
        "data_list = [\n",
        "    Data(\n",
        "        x=torch.rand((num_nodes, input_dim)),           # Node features\n",
        "        edge_index=torch.randint(0, num_nodes, (2, num_edges)),  # Edge indices\n",
        "        y=torch.rand((num_nodes, output_dim))            # Target values\n",
        "    )\n",
        "    for _ in range(100)  # Assume 100 samples\n",
        "]\n",
        "\n",
        "# Use this list of Data objects to create DataLoaders\n",
        "train_loader = DataLoader(data_list, batch_size=32, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(data_list, batch_size=32, shuffle=False, drop_last=True)\n",
        "\n",
        "'''\n",
        "train_loader = DataLoader(data_list, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(data_list, batch_size=32, shuffle=False)\n",
        "'''\n"
      ],
      "metadata": {
        "id": "6sjSsAP3GUGy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0f8f1e85-3630-4753-9bc8-345e18cc09b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntrain_loader = DataLoader(data_list, batch_size=32, shuffle=True)\\nval_loader = DataLoader(data_list, batch_size=32, shuffle=False)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'*DEBUGGING*"
      ],
      "metadata": {
        "id": "ujX4fXLeITfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for data_list in train_loader:\n",
        "    # Verify that `data_list` is a list of `Data` objects\n",
        "    print(f\"Type of data_list: {type(data_list)}\")\n",
        "    print(f\"Type of first item in data_list: {type(data_list[0])}\")\n",
        "\n",
        "    # Try to combine into a Batch and check the result\n",
        "    try:\n",
        "        data = Batch.from_data_list(data_list).to(device)\n",
        "        print(f\"Batched data type: {type(data)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during batching: {e}\")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "H0j5v_5VHwsr",
        "outputId": "15dc3e18-deb3-4730-d449-4c97eb31b963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor data_list in train_loader:\\n    # Verify that `data_list` is a list of `Data` objects\\n    print(f\"Type of data_list: {type(data_list)}\")\\n    print(f\"Type of first item in data_list: {type(data_list[0])}\")\\n\\n    # Try to combine into a Batch and check the result\\n    try:\\n        data = Batch.from_data_list(data_list).to(device)\\n        print(f\"Batched data type: {type(data)}\")\\n    except Exception as e:\\n        print(f\"Error during batching: {e}\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "for data_list in train_loader:\n",
        "    # Check the type of the batch and its contents\n",
        "    print(f\"Batch type: {type(data_list)}\")          # Should print <class 'list'>\n",
        "    print(f\"First item type in batch: {type(data_list[0])}\")  # Should print <class 'torch_geometric.data.data.Data'>\n",
        "\n",
        "    # If any unexpected types appear, this is where they might be introduced.\n",
        "    break  # Check only the first batch to reduce output\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "HL_c8XqlIXg6",
        "outputId": "dac4f841-ca24-44be-b349-513f797d1456"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor data_list in train_loader:\\n    # Check the type of the batch and its contents\\n    print(f\"Batch type: {type(data_list)}\")          # Should print <class \\'list\\'>\\n    print(f\"First item type in batch: {type(data_list[0])}\")  # Should print <class \\'torch_geometric.data.data.Data\\'>\\n\\n    # If any unexpected types appear, this is where they might be introduced.\\n    break  # Check only the first batch to reduce output\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "for data in train_loader:  # `data` is already a batched Data object\n",
        "    data = data.to(device)  # Move the entire batch to the device\n",
        "\n",
        "    # Run the inputs through GATNet\n",
        "    gat_output = gat_model(data)  # `data` contains `x`, `edge_index`, etc.\n",
        "\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8QDvu55bMK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c392ef2b-a6a2-4db1-e3e3-e827952f0e70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\nfor data in train_loader:  # `data` is already a batched Data object\\n    data = data.to(device)  # Move the entire batch to the device\\n\\n    # Run the inputs through GATNet\\n    gat_output = gat_model(data)  # `data` contains `x`, `edge_index`, etc.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TRAINING LOOP*"
      ],
      "metadata": {
        "id": "rB-Cx814IoTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import DataLoader, Batch\n",
        "\n",
        "# Define models, optimizer, etc.\n",
        "optimizer = optim.Adam(list(gat_model.parameters()) + list(transformer_model.parameters()), lr=0.0001)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "for data in train_loader:  # `data` is already a batched Data object\n",
        "    data = data.to(device)  # Move the batch to the device\n",
        "\n",
        "    # Step 1: Run the inputs through GATNet\n",
        "    gat_output = gat_model(data)  # Expects `data.x` and `data.edge_index`\n",
        "\n",
        "    # Step 2: Reshape or process `gat_output` as needed\n",
        "    # Assuming you need to reshape it for the Transformer input\n",
        "    batch_size = data.num_graphs\n",
        "    sequence_length = gat_output.size(0) // batch_size\n",
        "    gat_output = gat_output.view(batch_size, sequence_length, -1)\n",
        "\n",
        "    # Step 3: Run through Transformer\n",
        "    outputs = transformer_model(gat_output)\n",
        "\n",
        "    # Step 4: Compute loss\n",
        "    targets = data.y.to(device)  # Assuming `data.y` is the target\n",
        "    targets = targets.view(outputs.shape)  # Reshape targets to match outputs if needed\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backpropagation\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    '''\n"
      ],
      "metadata": {
        "id": "VYhcj55D5pPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "1a9caf0b-eb60-4fe6-9dae-b2909be6adde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom torch_geometric.data import DataLoader, Batch\\n\\n# Define models, optimizer, etc.\\noptimizer = optim.Adam(list(gat_model.parameters()) + list(transformer_model.parameters()), lr=0.0001)\\n\\n# Define the loss function\\ncriterion = nn.MSELoss()  # Mean Squared Error Loss for regression\\nfor data in train_loader:  # `data` is already a batched Data object\\n    data = data.to(device)  # Move the batch to the device\\n\\n    # Step 1: Run the inputs through GATNet\\n    gat_output = gat_model(data)  # Expects `data.x` and `data.edge_index`\\n\\n    # Step 2: Reshape or process `gat_output` as needed\\n    # Assuming you need to reshape it for the Transformer input\\n    batch_size = data.num_graphs\\n    sequence_length = gat_output.size(0) // batch_size\\n    gat_output = gat_output.view(batch_size, sequence_length, -1)\\n\\n    # Step 3: Run through Transformer\\n    outputs = transformer_model(gat_output)\\n\\n    # Step 4: Compute loss\\n    targets = data.y.to(device)  # Assuming `data.y` is the target\\n    targets = targets.view(outputs.shape)  # Reshape targets to match outputs if needed\\n    loss = criterion(outputs, targets)\\n\\n    # Backpropagation\\n    loss.backward()\\n    optimizer.step()\\n    '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(list(gat_model.parameters()) + list(transformer_model.parameters()), lr=0.0001)\n",
        "criterion = nn.MSELoss()  # Mean Squared Error Loss for regression\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Send models to the device\n",
        "gat_model.to(device)\n",
        "transformer_model.to(device)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # Set models to training mode\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}]\")   # Should output <class 'torch_geometric.data.Data'>\n",
        "\n",
        "    #Combine list of Data objects into a single Batch object\n",
        "    #data = Batch.from_data_list(data_list).to(device)\n",
        "    #data = data_list.to(device)\n",
        "    gat_model.train()\n",
        "    transformer_model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for data in train_loader:  # Iterate through batches (DataBatch objects) directly\n",
        "        # Move data to the device\n",
        "        data = data.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        # Step 1: Run the inputs through GATNet\n",
        "        gat_output = gat_model(data)  # Expects `data.x` and `data.edge_index` in GATNet\n",
        "\n",
        "        # Check the shape of gat_output\n",
        "        print(f\"GAT output shape: {gat_output.shape}\")\n",
        "\n",
        "        # Step 2: Reshape GAT output if needed\n",
        "        # Ensure gat_output has shape [batch_size, seq_len (num_nodes), embed_dim]\n",
        "        if len(gat_output.shape) == 2:  # If it’s [num_nodes * batch_size, embed_dim]\n",
        "            batch_size = data.num_graphs # Get number of graphs in the batch #changed to data.num_graphs\n",
        "            num_nodes = data.x.shape[0] // batch_size  # Compute nodes per graph\n",
        "            gat_output = gat_output.view(batch_size, num_nodes, -1)\n",
        "        # Check the reshaped shape\n",
        "        print(f\"Reshaped GAT output shape: {gat_output.shape}\")\n",
        "\n",
        "        # Step 3: Run the GAT output through Transformer directly\n",
        "        outputs = transformer_model(gat_output)\n",
        "\n",
        "        # Step 4: Compute loss between model output and targets\n",
        "        targets = data.y.to(device)  # Assuming targets are stored as `data.y`\n",
        "\n",
        "        # Check shapes of outputs and targets\n",
        "        print(f\"Outputs shape: {outputs.shape}\")\n",
        "        print(f\"Targets shape: {targets.shape}\")\n",
        "\n",
        "        targets = targets.view(outputs.shape)  # Reshape targets to match outputs shape\n",
        "\n",
        "        # Ensure targets have the same shape as outputs\n",
        "        if targets.shape != outputs.shape:\n",
        "            raise ValueError(f\"Targets shape {targets.shape} does not match Outputs shape {outputs.shape}\")\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print the average loss per epoch\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\n",
        "    # Validation phase\n",
        "    gat_model.eval()\n",
        "    transformer_model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # Disable gradients for validation\n",
        "        for data_list in val_loader:  # Expect each batch to be a list of Data objects\n",
        "            # Combine list of Data objects into a single Batch object\n",
        "            #data = Batch.from_data_list(data_list).to(device)\n",
        "            data = data_list.to(device)\n",
        "\n",
        "            # Run inputs through GAT and then Transformer\n",
        "            gat_output = gat_model(data)\n",
        "\n",
        "            # Reshape if needed\n",
        "            if len(gat_output.shape) == 2:\n",
        "                batch_size = data.num_graphs\n",
        "                num_nodes = data.x.shape[0] // batch_size\n",
        "                gat_output = gat_output.view(batch_size, num_nodes, -1)\n",
        "\n",
        "            # Get predictions from Transformer\n",
        "            targets = data.y.to(device)  # Assuming targets are stored as `data.y`\n",
        "            outputs = transformer_model(gat_output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDR-XRkZ1oSc",
        "outputId": "eae10a59-9a7a-48ef-96a6-300d996b72a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [1/10], Training Loss: 0.5095\n",
            "Epoch [2/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [2/10], Training Loss: 0.4136\n",
            "Epoch [3/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [3/10], Training Loss: 0.2731\n",
            "Epoch [4/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [4/10], Training Loss: 0.2439\n",
            "Epoch [5/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [5/10], Training Loss: 0.2143\n",
            "Epoch [6/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [6/10], Training Loss: 0.1682\n",
            "Epoch [7/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [7/10], Training Loss: 0.1704\n",
            "Epoch [8/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [8/10], Training Loss: 0.1502\n",
            "Epoch [9/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [9/10], Training Loss: 0.1574\n",
            "Epoch [10/10]\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "GAT output shape: torch.Size([1024, 32])\n",
            "Reshaped GAT output shape: torch.Size([32, 32, 32])\n",
            "Outputs shape: torch.Size([32, 32, 2])\n",
            "Targets shape: torch.Size([1024, 2])\n",
            "Epoch [10/10], Training Loss: 0.1407\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.MSELoss()  # Loss function for regression\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 50  # Adjust number of epochs based on model performance\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, targets)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Update weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)  # Average loss for the epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():  # No gradients needed during validation\n",
        "        for inputs, targets in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "'''"
      ],
      "metadata": {
        "id": "hhDp1PFL77Fv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "ff851a76-b41c-4a7b-edd0-0afc925562f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# Define the optimizer\\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\\ncriterion = nn.MSELoss()  # Loss function for regression\\n\\n# Training loop\\nnum_epochs = 50  # Adjust number of epochs based on model performance\\nfor epoch in range(num_epochs):\\n    model.train()  # Set model to training mode\\n    running_loss = 0.0\\n\\n    for inputs, targets in train_loader:\\n        optimizer.zero_grad()  # Zero the gradients\\n        outputs = model(inputs)  # Forward pass\\n        loss = criterion(outputs, targets)  # Compute loss\\n        loss.backward()  # Backward pass\\n        optimizer.step()  # Update weights\\n\\n        running_loss += loss.item()\\n\\n    avg_loss = running_loss / len(train_loader)  # Average loss for the epoch\\n    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\\n\\n    # Validation phase\\n    model.eval()  # Set model to evaluation mode\\n    val_loss = 0.0\\n    with torch.no_grad():  # No gradients needed during validation\\n        for inputs, targets in val_loader:\\n            outputs = model(inputs)\\n            loss = criterion(outputs, targets)\\n            val_loss += loss.item()\\n\\n    avg_val_loss = val_loss / len(val_loader)\\n    print(f'Validation Loss: {avg_val_loss:.4f}')\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Testing*"
      ],
      "metadata": {
        "id": "xc1DzkjAAb1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set the models to evaluation mode\n",
        "gat_model.eval()\n",
        "transformer_model.eval()\n",
        "\n",
        "# Placeholder to accumulate test loss\n",
        "test_loss = 0.0\n",
        "loss_fn = nn.MSELoss()  # Define the loss function (MSE for regression tasks)\n",
        "\n",
        "# Initialize lists to store predicted and actual values\n",
        "predicted_values = []\n",
        "actual_values = []\n",
        "\n",
        "# Evaluation loop\n",
        "with torch.no_grad():  # Disable gradient computation for faster testing\n",
        "    for data in val_loader:  # Assuming `val_loader` yields `Data` objects from PyTorch Geometric\n",
        "        data = data.to(device)\n",
        "\n",
        "        # Forward pass: Get predictions through GATNet and Transformer\n",
        "        gat_output = gat_model(data)  # Process input through GATNet\n",
        "        batch_size = data.num_graphs\n",
        "        seq_len = gat_output.size(0) // batch_size\n",
        "        gat_output = gat_output.view(batch_size, seq_len, -1)  # Reshape as needed\n",
        "        outputs = transformer_model(gat_output)  # Get predictions from Transformer\n",
        "\n",
        "        # Get targets\n",
        "        targets = data.y.to(device)\n",
        "        targets = targets.view(outputs.shape)  # Reshape targets to match outputs\n",
        "\n",
        "        # Compute test loss\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        # Store predictions and actual targets (convert tensors to numpy arrays)\n",
        "        predicted_values.append(outputs.cpu().numpy())  # Move to CPU and convert to numpy\n",
        "        actual_values.append(targets.cpu().numpy())  # Move to CPU and convert to numpy\n",
        "\n",
        "# Calculate the average test loss\n",
        "avg_test_loss = test_loss / len(val_loader)\n",
        "print(f\"Test Loss (MSE): {avg_test_loss:.4f}\")\n",
        "\n",
        "# Concatenate all predicted and actual values for further metrics\n",
        "predicted_values = np.concatenate(predicted_values, axis=0)  # Shape: (num_samples, seq_len, output_dim)\n",
        "actual_values = np.concatenate(actual_values, axis=0)  # Shape: (num_samples, seq_len, output_dim)\n",
        "\n",
        "# Calculate ADE (Average Displacement Error)\n",
        "displacement_errors = np.linalg.norm(predicted_values - actual_values, axis=-1)  # Euclidean distance per sample\n",
        "ade = np.mean(displacement_errors)\n",
        "print(f\"Average Displacement Error (ADE): {ade:.4f}\")\n",
        "\n",
        "# Calculate RMSE (Root Mean Squared Error)\n",
        "rmse = np.sqrt(np.mean((predicted_values - actual_values) ** 2))\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "\n",
        "# Display the first 5 predictions vs actuals for quick inspection\n",
        "print(\"\\nPredicted vs Actual (First 5 examples):\")\n",
        "for i in range(5):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(\"Predicted:\", predicted_values[i])\n",
        "    print(\"Actual:\", actual_values[i])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "vMJ_4qM__d3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd32050-3edf-422e-e992-30fabf0574bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss (MSE): 0.1087\n",
            "Average Displacement Error (ADE): 0.4243\n",
            "Root Mean Squared Error (RMSE): 0.3296\n",
            "\n",
            "Predicted vs Actual (First 5 examples):\n",
            "Example 1:\n",
            "Predicted: [[0.39138025 0.63601357]\n",
            " [0.5104768  0.17580643]\n",
            " [0.764047   0.54085356]\n",
            " [0.33442307 0.49663147]\n",
            " [0.5581514  0.41857898]\n",
            " [0.6197292  0.63103837]\n",
            " [0.8842747  0.70740944]\n",
            " [0.35283023 0.48059398]\n",
            " [0.4316411  0.5252736 ]\n",
            " [0.4756229  0.68911046]\n",
            " [0.63343906 0.4498633 ]\n",
            " [0.6499264  0.5487885 ]\n",
            " [0.9448458  0.54765534]\n",
            " [0.9329139  0.7774202 ]\n",
            " [0.52759266 0.7255242 ]\n",
            " [0.427374   0.4291444 ]\n",
            " [0.48069316 0.5880166 ]\n",
            " [0.33229685 0.67572665]\n",
            " [0.6451483  0.5464324 ]\n",
            " [0.18817313 0.6280447 ]\n",
            " [0.48946398 0.45602003]\n",
            " [0.4364801  0.1547232 ]\n",
            " [0.72730577 0.6233667 ]\n",
            " [0.5019367  0.50877845]\n",
            " [0.25424856 0.6598622 ]\n",
            " [0.4021685  0.47973296]\n",
            " [0.38003284 0.5355037 ]\n",
            " [0.51296026 0.58993036]\n",
            " [0.7822957  0.15784436]\n",
            " [0.3096804  0.18867874]\n",
            " [0.5490605  0.5954328 ]\n",
            " [0.8591065  0.03364644]]\n",
            "Actual: [[2.20122755e-01 6.87700808e-01]\n",
            " [6.06140196e-01 9.06694889e-01]\n",
            " [6.29038036e-01 6.28505349e-01]\n",
            " [3.88878763e-01 7.31847942e-01]\n",
            " [4.02961910e-01 6.66361392e-01]\n",
            " [9.58244026e-01 5.39351285e-01]\n",
            " [1.46346688e-02 6.81772649e-01]\n",
            " [7.66789854e-01 4.34381008e-01]\n",
            " [9.26291287e-01 5.69923282e-01]\n",
            " [3.45519900e-01 8.94131839e-01]\n",
            " [7.31468201e-04 9.24634516e-01]\n",
            " [8.80050957e-01 7.14575469e-01]\n",
            " [9.31794047e-01 2.60744035e-01]\n",
            " [6.52958035e-01 6.37649059e-01]\n",
            " [6.33976460e-01 4.25343037e-01]\n",
            " [6.34285212e-02 5.76600790e-01]\n",
            " [3.32629502e-01 5.27882397e-01]\n",
            " [5.81358135e-01 8.11712742e-02]\n",
            " [1.48686707e-01 4.71518040e-01]\n",
            " [2.68413842e-01 4.77901399e-01]\n",
            " [7.72375941e-01 1.08287215e-01]\n",
            " [9.30499434e-02 2.51564205e-01]\n",
            " [1.46623969e-01 8.74969661e-01]\n",
            " [1.99271679e-01 2.76815593e-01]\n",
            " [3.51022720e-01 7.10872114e-01]\n",
            " [6.40348494e-01 1.64649725e-01]\n",
            " [8.15820634e-01 4.64529097e-01]\n",
            " [5.97231984e-01 5.91446280e-01]\n",
            " [8.40490460e-02 6.54945135e-01]\n",
            " [8.54469061e-01 3.00654769e-02]\n",
            " [8.44205797e-01 4.23158407e-02]\n",
            " [6.16589844e-01 3.18707824e-02]]\n",
            "\n",
            "Example 2:\n",
            "Predicted: [[0.39133346 0.6351157 ]\n",
            " [0.51089007 0.1744155 ]\n",
            " [0.7650545  0.54007703]\n",
            " [0.33476162 0.49589056]\n",
            " [0.5605445  0.41669413]\n",
            " [0.62140656 0.6290623 ]\n",
            " [0.8865155  0.7065253 ]\n",
            " [0.3537643  0.4794789 ]\n",
            " [0.4318236  0.5252293 ]\n",
            " [0.47581035 0.68853915]\n",
            " [0.6335668  0.4487559 ]\n",
            " [0.64979315 0.5476458 ]\n",
            " [0.94585884 0.54733115]\n",
            " [0.9338398  0.7770818 ]\n",
            " [0.52831346 0.7247625 ]\n",
            " [0.42740345 0.4284146 ]\n",
            " [0.48013937 0.5871643 ]\n",
            " [0.3318658  0.6740319 ]\n",
            " [0.6458145  0.5456275 ]\n",
            " [0.18823163 0.6264967 ]\n",
            " [0.4895382  0.4555036 ]\n",
            " [0.43766326 0.15134501]\n",
            " [0.7277953  0.6221606 ]\n",
            " [0.5030677  0.507734  ]\n",
            " [0.2550177  0.6591969 ]\n",
            " [0.40231556 0.47881338]\n",
            " [0.38005012 0.5345478 ]\n",
            " [0.5131985  0.5888051 ]\n",
            " [0.7828242  0.15677994]\n",
            " [0.31056708 0.18768018]\n",
            " [0.55188644 0.5948373 ]\n",
            " [0.8600273  0.03213087]]\n",
            "Actual: [[0.9595711  0.7697643 ]\n",
            " [0.8198449  0.3897863 ]\n",
            " [0.00817329 0.14380366]\n",
            " [0.44482523 0.85349256]\n",
            " [0.65791416 0.08874822]\n",
            " [0.52547246 0.46083802]\n",
            " [0.26486284 0.24891049]\n",
            " [0.0286203  0.30945516]\n",
            " [0.9478294  0.35481346]\n",
            " [0.42778492 0.73109907]\n",
            " [0.78818905 0.2503335 ]\n",
            " [0.2608745  0.33002597]\n",
            " [0.9008494  0.5927565 ]\n",
            " [0.02466613 0.51052517]\n",
            " [0.34026945 0.68053865]\n",
            " [0.2222681  0.84770626]\n",
            " [0.38545847 0.22034252]\n",
            " [0.95237964 0.93842256]\n",
            " [0.92989606 0.46885914]\n",
            " [0.01722169 0.6573265 ]\n",
            " [0.57446545 0.7369045 ]\n",
            " [0.9259687  0.542964  ]\n",
            " [0.23280185 0.50989527]\n",
            " [0.6193958  0.9608997 ]\n",
            " [0.33048767 0.38097084]\n",
            " [0.4301681  0.30372143]\n",
            " [0.9225808  0.9154299 ]\n",
            " [0.77056813 0.9103824 ]\n",
            " [0.8092009  0.7539923 ]\n",
            " [0.919819   0.33435816]\n",
            " [0.6115039  0.10356861]\n",
            " [0.87451315 0.5694281 ]]\n",
            "\n",
            "Example 3:\n",
            "Predicted: [[0.39082223 0.6351627 ]\n",
            " [0.51068985 0.17448682]\n",
            " [0.76445276 0.541121  ]\n",
            " [0.33477008 0.49641725]\n",
            " [0.5589539  0.41764024]\n",
            " [0.6206982  0.6304025 ]\n",
            " [0.88584733 0.70679235]\n",
            " [0.3539815  0.47989216]\n",
            " [0.43231493 0.5253271 ]\n",
            " [0.47492194 0.6882856 ]\n",
            " [0.63305354 0.44941825]\n",
            " [0.64966965 0.54783875]\n",
            " [0.9456959  0.5476446 ]\n",
            " [0.9333714  0.77697057]\n",
            " [0.5275068  0.72507936]\n",
            " [0.4267435  0.42897823]\n",
            " [0.4798321  0.5871672 ]\n",
            " [0.33041418 0.6749309 ]\n",
            " [0.6451487  0.5465063 ]\n",
            " [0.18791641 0.6267621 ]\n",
            " [0.48917097 0.4557205 ]\n",
            " [0.43764484 0.15224546]\n",
            " [0.72822446 0.6227336 ]\n",
            " [0.50274956 0.50737387]\n",
            " [0.25496286 0.6597203 ]\n",
            " [0.40226895 0.47911286]\n",
            " [0.3801124  0.53506285]\n",
            " [0.5131827  0.5888291 ]\n",
            " [0.78222567 0.157556  ]\n",
            " [0.30999768 0.18767905]\n",
            " [0.55113673 0.5956283 ]\n",
            " [0.8591989  0.03152714]]\n",
            "Actual: [[0.76803523 0.37197602]\n",
            " [0.94708675 0.8268709 ]\n",
            " [0.07760274 0.95131946]\n",
            " [0.724525   0.93034315]\n",
            " [0.93059635 0.9069387 ]\n",
            " [0.34558833 0.8974358 ]\n",
            " [0.95626754 0.44317782]\n",
            " [0.04785746 0.24558622]\n",
            " [0.17721176 0.028171  ]\n",
            " [0.68504006 0.28628212]\n",
            " [0.42867392 0.09293091]\n",
            " [0.41659874 0.69573885]\n",
            " [0.00497526 0.90069294]\n",
            " [0.18986589 0.80070096]\n",
            " [0.64052194 0.35207015]\n",
            " [0.4957587  0.2919559 ]\n",
            " [0.16935748 0.10584486]\n",
            " [0.81281424 0.9472606 ]\n",
            " [0.55420494 0.2485528 ]\n",
            " [0.71043813 0.8165689 ]\n",
            " [0.6008584  0.7299492 ]\n",
            " [0.47709274 0.6418457 ]\n",
            " [0.3231256  0.04462159]\n",
            " [0.18601012 0.6835606 ]\n",
            " [0.11708665 0.32616472]\n",
            " [0.12649542 0.80089307]\n",
            " [0.05168766 0.39010578]\n",
            " [0.6983675  0.3898492 ]\n",
            " [0.64269114 0.52248156]\n",
            " [0.9088856  0.7046057 ]\n",
            " [0.6533515  0.12397754]\n",
            " [0.14678448 0.6737899 ]]\n",
            "\n",
            "Example 4:\n",
            "Predicted: [[0.39085364 0.635093  ]\n",
            " [0.51061004 0.17465052]\n",
            " [0.76413596 0.540317  ]\n",
            " [0.33462292 0.49581143]\n",
            " [0.5589298  0.41769576]\n",
            " [0.6201913  0.6296339 ]\n",
            " [0.8854656  0.7070075 ]\n",
            " [0.3532833  0.4789155 ]\n",
            " [0.43092    0.5250145 ]\n",
            " [0.4747103  0.688486  ]\n",
            " [0.6332364  0.44882032]\n",
            " [0.64952433 0.5475235 ]\n",
            " [0.9446125  0.54762536]\n",
            " [0.9332893  0.77736366]\n",
            " [0.5273457  0.7247634 ]\n",
            " [0.42646998 0.4283881 ]\n",
            " [0.47999072 0.5874208 ]\n",
            " [0.3302167  0.6744031 ]\n",
            " [0.6448961  0.54592717]\n",
            " [0.1875994  0.62637305]\n",
            " [0.48896587 0.4561654 ]\n",
            " [0.4371779  0.15142313]\n",
            " [0.7273192  0.6224393 ]\n",
            " [0.50227946 0.50729567]\n",
            " [0.25458747 0.6594671 ]\n",
            " [0.4021939  0.47871676]\n",
            " [0.37992096 0.53463286]\n",
            " [0.5129123  0.5885939 ]\n",
            " [0.78248495 0.15708393]\n",
            " [0.31017888 0.18754283]\n",
            " [0.5510296  0.5946534 ]\n",
            " [0.85880566 0.0321312 ]]\n",
            "Actual: [[0.5215576  0.8674    ]\n",
            " [0.9671561  0.00736815]\n",
            " [0.1415053  0.6339265 ]\n",
            " [0.6088333  0.5043376 ]\n",
            " [0.09286749 0.95412016]\n",
            " [0.09198081 0.4105062 ]\n",
            " [0.5685126  0.570486  ]\n",
            " [0.48694474 0.549604  ]\n",
            " [0.71559936 0.14458364]\n",
            " [0.07376182 0.5435401 ]\n",
            " [0.12920076 0.7858154 ]\n",
            " [0.60864365 0.56508523]\n",
            " [0.37982625 0.8294901 ]\n",
            " [0.6271534  0.49649245]\n",
            " [0.11644065 0.32068145]\n",
            " [0.16009831 0.3319068 ]\n",
            " [0.39866287 0.17579901]\n",
            " [0.43186247 0.6203308 ]\n",
            " [0.6700684  0.22707564]\n",
            " [0.03272593 0.564077  ]\n",
            " [0.95484537 0.4381451 ]\n",
            " [0.00416118 0.3563128 ]\n",
            " [0.07647264 0.23827481]\n",
            " [0.44380057 0.63717103]\n",
            " [0.25994837 0.8025998 ]\n",
            " [0.31908303 0.9829575 ]\n",
            " [0.23919576 0.56515247]\n",
            " [0.30515856 0.8473414 ]\n",
            " [0.56612056 0.21273232]\n",
            " [0.5029763  0.18370837]\n",
            " [0.2319501  0.9454999 ]\n",
            " [0.06328863 0.95518553]]\n",
            "\n",
            "Example 5:\n",
            "Predicted: [[0.39102936 0.6346902 ]\n",
            " [0.5110279  0.17382881]\n",
            " [0.7658747  0.5395052 ]\n",
            " [0.33490843 0.49560586]\n",
            " [0.55985063 0.4170173 ]\n",
            " [0.6213499  0.62861985]\n",
            " [0.88713425 0.70582694]\n",
            " [0.35425967 0.47872013]\n",
            " [0.43239832 0.52423424]\n",
            " [0.47552776 0.6881274 ]\n",
            " [0.63342625 0.44868106]\n",
            " [0.64988947 0.5466301 ]\n",
            " [0.9462669  0.5467997 ]\n",
            " [0.9322902  0.77916527]\n",
            " [0.5285377  0.72466856]\n",
            " [0.42667365 0.42784497]\n",
            " [0.48074162 0.5869786 ]\n",
            " [0.33168578 0.6738401 ]\n",
            " [0.646064   0.5454886 ]\n",
            " [0.18846895 0.6258615 ]\n",
            " [0.48992437 0.45499223]\n",
            " [0.43794715 0.15072131]\n",
            " [0.72815025 0.6219553 ]\n",
            " [0.50331634 0.50700563]\n",
            " [0.2549296  0.65899277]\n",
            " [0.40224236 0.47830707]\n",
            " [0.38007778 0.5345268 ]\n",
            " [0.51325756 0.5883244 ]\n",
            " [0.78283334 0.156508  ]\n",
            " [0.31059813 0.18742114]\n",
            " [0.552179   0.59435356]\n",
            " [0.85986996 0.03132192]]\n",
            "Actual: [[0.4147011  0.08771205]\n",
            " [0.18236488 0.38766623]\n",
            " [0.08839464 0.6367689 ]\n",
            " [0.02501601 0.31876546]\n",
            " [0.17885292 0.86910766]\n",
            " [0.04581636 0.9857136 ]\n",
            " [0.14299119 0.55034035]\n",
            " [0.884634   0.07331651]\n",
            " [0.8750857  0.16115528]\n",
            " [0.43370283 0.51176697]\n",
            " [0.25859427 0.43025517]\n",
            " [0.79553926 0.5446001 ]\n",
            " [0.72739214 0.83725363]\n",
            " [0.06519532 0.14147872]\n",
            " [0.42551494 0.25548357]\n",
            " [0.1733846  0.4991116 ]\n",
            " [0.84526104 0.84501386]\n",
            " [0.4167248  0.34842217]\n",
            " [0.98144335 0.8494124 ]\n",
            " [0.29709983 0.7945258 ]\n",
            " [0.57396847 0.9608331 ]\n",
            " [0.49878347 0.6081733 ]\n",
            " [0.1452725  0.6517296 ]\n",
            " [0.20095748 0.6125106 ]\n",
            " [0.3078757  0.30452514]\n",
            " [0.09749699 0.1472441 ]\n",
            " [0.24079055 0.70496994]\n",
            " [0.51575637 0.2834472 ]\n",
            " [0.8110409  0.5041761 ]\n",
            " [0.38342106 0.3388046 ]\n",
            " [0.9487573  0.02937502]\n",
            " [0.97005224 0.77642465]]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}